{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using Pre-trained BERT from Hugging Face\n",
    "\n",
    "This notebook demonstrates how to use pre-trained BERT models from Hugging Face's transformers library."
   ],
   "id": "2f4afc7c9b81fe8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T03:58:36.816191Z",
     "start_time": "2025-05-06T03:58:20.656183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "#!pip install transformers torch\n",
    "\n",
    "# Import necessary libraries\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text = \"Here's a sample sentence to encode using BERT.\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Get BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the embeddings from the last hidden state\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(\"Input shape:\", inputs['input_ids'].shape)\n",
    "print(\"Output shape (last hidden states):\", last_hidden_states.shape)\n",
    "\n",
    "# Example of getting sentence embedding (using mean pooling)\n",
    "sentence_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "print(\"Sentence embedding shape:\", sentence_embedding.shape)"
   ],
   "id": "9fa20f0bc6eb15e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a52edde768c4da98aa13a558f5f45a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cca6ec32f56e43c5bbeeb2c4be961e28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e92b657b8a5b4b8a8843a465c1b96797"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35d4ced98bae45aab917405b66ec1b85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "577cfaf9b9ab41efacb6e44af4091582"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 14])\n",
      "Output shape (last hidden states): torch.Size([1, 14, 768])\n",
      "Sentence embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:00:41.597828Z",
     "start_time": "2025-05-06T04:00:41.180078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load model for classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\"Text to classify\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits"
   ],
   "id": "a2df2932bdaeebc9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:00:53.626837Z",
     "start_time": "2025-05-06T04:00:53.339685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "# Load QA model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare input\n",
    "question = \"Who is the author?\"\n",
    "context = \"The book was written by Jane Doe.\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Get answer\n",
    "outputs = model(**inputs)\n",
    "answer_start = outputs.start_logits\n",
    "answer_end = outputs.end_logits"
   ],
   "id": "3dfaf2915c1a5ee8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea2d920c52a2c67d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
