{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## [Download the IMDB Dataset](https://medium.com/@sayedebad.777/building-rnn-lstm-and-gru-from-scratch-e27ebb0f8e0f)",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:19:42.897114Z",
     "start_time": "2025-03-11T04:19:42.894446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from keras.src.utils.module_utils import tensorflow\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(tensorflow.__version__)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "2.18.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:21:26.006654Z",
     "start_time": "2025-03-11T04:21:17.776652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the maximum number of words to include in the vocabulary\n",
    "max_features = 5000\n",
    "\n",
    "# Define the maximum length to which sequences will be padded\n",
    "max_len = 500\n",
    "\n",
    "# Load the IMDB dataset with the specified vocabulary size\n",
    "# x_train, y_train: Training data and labels\n",
    "# x_test, y_test: Test data and labels\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ],
   "id": "6a58b55e9bbc2fc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001B[1m17464789/17464789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 0us/step\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:30:56.924157Z",
     "start_time": "2025-03-11T04:30:56.639138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pad sequences to ensure uniform length \n",
    "# (ensures all input data samples have the same sequence length for model training)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# Convert the padded sequences and their corresponding labels into PyTorch tensors \n",
    "# (prepares data for usage in PyTorch DataLoader and models)\n",
    "x_train = torch.as_tensor(x_train, dtype=torch.long)\n",
    "y_train = torch.as_tensor(y_train, dtype=torch.float32)\n",
    "x_test = torch.as_tensor(x_test, dtype=torch.long)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.float32)"
   ],
   "id": "fe35983694dfdf63",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:32:25.119431Z",
     "start_time": "2025-03-11T04:32:25.112722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "# TensorDataset groups multiple tensors into a dataset. \n",
    "# Here, it combines the input data (x_train/x_test) and the labels (y_train/y_test).\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "\n",
    "# DataLoader is used to efficiently handle batching and shuffling of data during training and testing.\n",
    "# Parameters: batch_size specifies the size of the batches, shuffle randomizes the dataset order.\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ],
   "id": "9441740a4d0cc5ab",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RNN Model",
   "id": "1f1bde766536e3d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:38:34.755714Z",
     "start_time": "2025-03-11T04:38:34.277108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the RNNModel class, a PyTorch neural network model for sentiment analysis\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Embedding layer for converting word indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # RNN (Recurrent Neural Network) layer with specified dimensions and number of layers\n",
    "        # batch_first=True ensures input batches have shape (batch_size, sequence_length, features)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        # Fully connected layer to map hidden states to the output dimension\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # Sigmoid activation function to produce probabilities for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the embedding layer to get word embeddings\n",
    "        x = self.embedding(x)\n",
    "        # Pass the embeddings through the RNN layer; only use the last hidden state\n",
    "        x, _ = self.rnn(x)\n",
    "        # Extract the last time step's output\n",
    "        x = x[:, -1, :]\n",
    "        # Pass the last hidden state through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # Apply the sigmoid activation to get the final output (binary probability)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define model hyperparameters\n",
    "vocab_size = max_features  # Vocabulary size, i.e., max number of unique tokens\n",
    "embedding_dim = 128  # Dimension of word embeddings\n",
    "hidden_dim = 128  # Dimension of hidden state in the RNN\n",
    "output_dim = 1  # Single output for binary classification (positive/negative sentiment)\n",
    "num_layers = 3  # Number of layers for the RNN\n",
    "\n",
    "# Initialize the model with defined hyperparameters and send it to the device (CPU or GPU)\n",
    "rnn_model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers).to(device)\n",
    "\n",
    "# Define binary cross-entropy loss for binary classification tasks\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer (Adam) for updating model parameters during training\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)"
   ],
   "id": "fa7c1300793460e7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T05:01:48.886672Z",
     "start_time": "2025-03-11T04:41:11.721043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "num_epochs = 5  # Define the number of epochs for training\n",
    "\n",
    "for epoch in range(num_epochs):  # Loop over each epoch\n",
    "    rnn_model.train()  # Set the model to training mode\n",
    "    total_loss = 0  # Initialize the total loss for this epoch\n",
    "    for inputs, targets in train_loader:  # Iterate over batches in the training data\n",
    "        inputs = inputs.to(device)  # Move input data to the specified device (CPU/GPU)\n",
    "        targets = targets.to(device)  # Move target labels to the specified device\n",
    "        optimizer.zero_grad()  # Reset gradients to zero before backpropagation\n",
    "        outputs = rnn_model(inputs)  # Perform the forward pass through the RNN model\n",
    "        loss = criterion(outputs.squeeze(), targets)  # Compute the binary cross-entropy loss\n",
    "        loss.backward()  # Backpropagate the loss to calculate gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "        total_loss += loss.item()  # Accumulate the batch loss into the total loss\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)  # Calculate average loss over all batches\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')  # Print the epoch loss"
   ],
   "id": "1bfcc7f238efa6d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6615\n",
      "Epoch [2/5], Loss: 0.6542\n",
      "Epoch [3/5], Loss: 0.6153\n",
      "Epoch [4/5], Loss: 0.5841\n",
      "Epoch [5/5], Loss: 0.6357\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T05:04:17.649537Z",
     "start_time": "2025-03-11T05:03:54.968658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "rnn_model.eval()  # Set the model to evaluation mode (disables features like dropout)\n",
    "correct, total = 0, 0  # Initialize counters for correct predictions and total samples\n",
    "with torch.no_grad():  # Disable gradient computation for memory efficiency during evaluation\n",
    "    for inputs, targets in test_loader:  # Iterate over batches from the test DataLoader\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move inputs and targets to the device (CPU/GPU)\n",
    "        outputs = rnn_model(inputs)  # Perform forward pass to get model predictions\n",
    "        predicted = (outputs.squeeze() > 0.5).float()  # Apply threshold (0.5) to generate binary predictions\n",
    "        total += targets.size(0)  # Add the number of samples in the current batch to the total\n",
    "        correct += (predicted == targets).sum().item()  # Count the correctly predicted samples\n",
    "\n",
    "accuracy = correct / total  # Calculate overall accuracy as the ratio of correct predictions to total samples\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}% - {accuracy:.4f}')  # Print accuracy in percentage and decimal format"
   ],
   "id": "58897a72d634a19e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 60.12% - 0.6012\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM Model",
   "id": "a8bc0ba8e01f2fc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T23:23:59.968903Z",
     "start_time": "2025-03-11T23:23:59.963653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the LSTMModel class for text classification\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embdding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Embedding layer for mapping word indices to dense vector representations\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM layer for sequential data processing\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        # Fully connected layer for mapping hidden state features to output\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # Sigmoid activation function for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute word embeddings\n",
    "        x = self.embedding(x)\n",
    "        # Process embeddings with the LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        # Extract features from the last time-step\n",
    "        x = x[:, -1, :]\n",
    "        # Apply the fully connected layer to the extracted features\n",
    "        x = self.fc(x)\n",
    "        # Apply the sigmoid activation to get probabilities\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "vocab_size = max_features  # Size of the vocabulary (number of unique tokens to use)\n",
    "embedding_dim = 128  # Dimensionality of the embeddings for each token\n",
    "hidden_dim = 128  # Dimensionality of the hidden state in the LSTM\n",
    "output_dim = 1  # Single output for binary classification (positive/negative sentiment)\n",
    "num_layers = 3  # Number of stacked LSTM layers in the network\n",
    "\n",
    "# Initialize the model, send it to the available device (CPU or GPU)\n",
    "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers).to(device)\n",
    "\n",
    "# Define the loss function to measure the model's performance (Binary Cross-Entropy Loss)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer for parameter updates during training (Adam optimizer)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)"
   ],
   "id": "1a6289df6bc146ac",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T23:53:18.170326Z",
     "start_time": "2025-03-11T23:26:16.070020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop for the LSTM model\n",
    "# num_epochs defines the number of full passes through the dataset for training\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):  # Iterate over epochs\n",
    "    lstm_model.train()  # Set the model to training mode (enables layers like dropout if present)\n",
    "    total_loss = 0  # Initialize variable to accumulate total loss for the epoch\n",
    "\n",
    "    # Iterate over batches of data from the training DataLoader\n",
    "    for inputs, targets in train_loader:\n",
    "        # Move inputs and targets to the device (CPU/GPU) to facilitate computation\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Reset gradients to ensure they don’t accumulate from previous steps\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the model to get predictions\n",
    "        outputs = lstm_model(inputs)\n",
    "\n",
    "        # Compute the loss between predictions and actual targets\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "\n",
    "        # Perform backpropagation to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters based on computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the current batch's loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute the average loss over all batches in the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Print the loss for the epoch to monitor training progress\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ],
   "id": "47df04fdb584d547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5862\n",
      "Epoch [2/5], Loss: 0.4440\n",
      "Epoch [3/5], Loss: 0.3117\n",
      "Epoch [4/5], Loss: 0.2437\n",
      "Epoch [5/5], Loss: 0.1956\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T23:54:03.053008Z",
     "start_time": "2025-03-11T23:53:18.234382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model's performance on the test dataset\n",
    "lstm_model.eval()  # Switch the model to evaluation mode, disabling dropout and other training-only behaviors\n",
    "correct = 0  # Initialize a counter for the number of correct predictions\n",
    "total = 0  # Initialize a counter for the total number of samples\n",
    "\n",
    "# Disable gradient computation during evaluation to save memory and improve computation speed\n",
    "with torch.no_grad():\n",
    "    # Iterate through the test data batches\n",
    "    for inputs, targets in test_loader:\n",
    "        # Move input data and target labels to the selected device (CPU or GPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Perform a forward pass through the LSTM model to get the predicted probabilities\n",
    "        outputs = lstm_model(inputs)\n",
    "\n",
    "        # Apply a threshold of 0.5 to convert probabilities into binary predictions (0 or 1)\n",
    "        predicted = (outputs.squeeze() >= 0.5).float()\n",
    "\n",
    "        # Update the total number of samples and count correct predictions\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "# Compute the overall test accuracy as the ratio of correctly predicted samples to the total samples\n",
    "accuracy = correct / total\n",
    "# Print the test accuracy, formatted to display four decimal places\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ],
   "id": "efc5b0cab3a3d989",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8810\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GRU Model",
   "id": "b909675b09fb3515"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T01:53:41.504573Z",
     "start_time": "2025-03-12T01:53:41.499347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        # Embedding layer maps integer-encoded words to dense vectors of a fixed size (embedding_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # GRU layer processes input sequences to capture temporal dependencies.\n",
    "        # embedding_dim is the input size, hidden_dim is the output size of each GRU cell, \n",
    "        # and num_layers specifies the number of stacked GRU layers.\n",
    "        # batch_first=True ensures input tensors have the shape (batch_size, seq_len, features).\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer maps the final hidden state of the GRU to the desired output dimension.\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Activation function (Sigmoid) outputs probabilities for binary classification.\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Convert input word indices into dense embeddings.\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Step 2: Process embeddings with the GRU layer; the (_, _) outputs are hidden states.\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        # Step 3: Extract the last hidden state across the sequence (x[:, -1, :]).\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Step 4: Pass the final hidden state through the fully connected layer.\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # Step 5: Apply the sigmoid activation function to the output.\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model hyperparameters (parameters for model configuration):\n",
    "vocab_size = max_features  # Size of the vocabulary (number of unique tokens to use in embeddings).\n",
    "embedding_dim = 128  # Number of dimensions in the embedding vectors.\n",
    "hidden_dim = 128  # Number of hidden units in the GRU cell.\n",
    "output_dim = 1  # Single output for binary classification (positive or negative sentiment).\n",
    "num_layers = 3  # Number of stacked GRU layers.\n",
    "\n",
    "# Initialize the GRU model with the defined hyperparameters.\n",
    "gru_model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define the loss function: Binary Cross Entropy (suitable for binary classification tasks).\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer: Adam optimizer adjusts the GRU model’s parameters to minimize the loss.\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)"
   ],
   "id": "bb4d355b4325c855",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T02:20:02.243567Z",
     "start_time": "2025-03-12T01:56:14.266287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "num_epochs = 5  # Define the number of epochs (full passes through training data).\n",
    "\n",
    "for epoch in range(num_epochs):  # Outer loop over epochs, controlling the progress of training.\n",
    "    gru_model.train()  # Set the GRU model to training mode (enables features like dropout if applied).\n",
    "\n",
    "    for inputs, targets in train_loader:  # Inner loop iterating over batches of training data.\n",
    "        inputs, targets = inputs.to(device), targets.to(\n",
    "            device)  # Move inputs and targets to the computation device (CPU/GPU).\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step to ensure they don't accumulate.\n",
    "        outputs = gru_model(inputs)  # Perform a forward pass through the model to compute predictions.\n",
    "        loss = criterion(outputs.squeeze(),\n",
    "                         targets)  # Calculate the loss (Binary Cross Entropy) based on model outputs and target labels.\n",
    "        loss.backward()  # Backpropagate the computed loss to calculate gradients for all model parameters.\n",
    "        optimizer.step()  # Update model parameters using the computed gradients and optimizer.\n",
    "        total_loss += loss.item()  # Accumulate the current batch's loss into the total loss for the epoch.\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)  # Compute the average loss across all training batches.\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')  # Output the average training loss for this epoch."
   ],
   "id": "d14f295c25fb9dbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7172\n",
      "Epoch [2/5], Loss: 1.0077\n",
      "Epoch [3/5], Loss: 1.2188\n",
      "Epoch [4/5], Loss: 1.3746\n",
      "Epoch [5/5], Loss: 1.4816\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T02:21:13.699999Z",
     "start_time": "2025-03-12T02:20:02.299822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "gru_model.eval()  # Set the GRU model to evaluation mode (disables dropout and similar layers).\n",
    "correct = 0  # Initialize a counter for correct predictions.\n",
    "total = 0  # Initialize a counter for the total number of samples.\n",
    "\n",
    "# Disable gradient computation for evaluation to save memory and computational resources.\n",
    "with torch.no_grad():\n",
    "    # Loop through batches from the test DataLoader to compute predictions.\n",
    "    for inputs, targets in test_loader:\n",
    "        # Move the inputs and targets to the specified device (CPU/GPU).\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass: compute the model's output probabilities for the current batch.\n",
    "        outputs = gru_model(inputs)\n",
    "\n",
    "        # Apply a threshold to the output probabilities to generate binary predictions.\n",
    "        predicted = (outputs.squeeze() >= 0.5).float()\n",
    "\n",
    "        # Update the total with the batch size.\n",
    "        total += targets.size(0)\n",
    "\n",
    "        # Count the number of correct predictions in the current batch.\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "# Calculate accuracy as the ratio of correct predictions to the total samples.\n",
    "accuracy = correct / total\n",
    "\n",
    "# Print the test accuracy formatted to four decimal places.\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ],
   "id": "d9a18ab796bbc60c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8922\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "50ebf75191052182"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
