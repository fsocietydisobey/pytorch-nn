{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T04:21:34.558073Z",
     "start_time": "2025-02-24T04:21:34.555850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.cluster import DBSCAN"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T17:29:27.057591Z",
     "start_time": "2025-02-23T17:29:26.991227Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": [
    "# Initialize MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load an image containing faces\n",
    "img = Image.open('data/images/img_1.jpg')\n",
    "\n",
    "# Detect faces in the image\n",
    "boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "# If faces are detected, 'boxes' will contain the bounding box coordinates\n",
    "if boxes is not None:\n",
    "    for box in boxes:\n",
    "        # Draw bounding boxes on the image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle(box.tolist(), outline='red', width=3)\n",
    "\n",
    "# Display or save the image with detected faces\n",
    "img.show()"
   ],
   "id": "ac54e90e5974bfd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T17:44:16.604268Z",
     "start_time": "2025-02-23T17:44:16.369772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize MTCNN and ResNet models\n",
    "mtcnn = MTCNN()  # MTCNN is used for detecting and aligning faces in images\n",
    "resnet = InceptionResnetV1(pretrained='casia-webface').eval()  # Pre-trained Inception ResNet model for face embeddings\n",
    "\n",
    "# Load an image (replace 'your_image.jpg' with the actual image path)\n",
    "img = Image.open('data/images/img_1.jpg')  # Open the image using PIL\n",
    "\n",
    "# Preprocess the image and extract embeddings\n",
    "aligned = mtcnn(img)  # Align the detected face(s) in the image\n",
    "\n",
    "if aligned is not None:  # Check if any face was detected and aligned\n",
    "    print(aligned.shape)  # Print the shape of the aligned face tensor\n",
    "\n",
    "    # Add batch dimension to the aligned image (necessary for model input)\n",
    "    aligned = aligned.unsqueeze(0)  # Reshape to [1, 3, 160, 160]\n",
    "\n",
    "    # Pass the preprocessed image to the ResNet model to get face embeddings\n",
    "    embeddings = resnet(aligned).detach()  # Detach the tensor from the computation graph\n",
    "\n",
    "    # 'embeddings' now contains the feature vector for the detected face\n",
    "    print(embeddings, embeddings.shape)  # Print the embeddings and their shape for reference\n",
    "else:\n",
    "    # If no face is detected in the image, print a message\n",
    "    print(\"No face detected in the image.\")"
   ],
   "id": "384655fdf00cb4e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 160, 160])\n",
      "tensor([[ 1.9026e-02, -6.0987e-02,  7.1703e-03, -4.8106e-02,  3.0901e-02,\n",
      "         -5.6444e-02,  7.2428e-02,  8.5632e-02,  1.3193e-02, -7.1483e-02,\n",
      "          4.8667e-02, -7.1583e-02, -6.2817e-03,  5.6666e-02, -5.6190e-02,\n",
      "          8.5444e-02,  7.4032e-02, -9.9748e-02,  2.7052e-02,  4.9049e-02,\n",
      "         -2.8160e-02, -1.7794e-02,  1.4784e-03,  1.2196e-02, -3.4382e-02,\n",
      "         -1.2122e-02,  3.4515e-02,  6.4010e-03,  9.1089e-02, -6.9073e-02,\n",
      "          1.4593e-02,  2.5911e-02,  2.3787e-02,  2.4444e-03,  1.0286e-02,\n",
      "         -1.4961e-02, -3.1552e-04,  2.0226e-02, -5.9818e-02,  2.9698e-02,\n",
      "          4.3853e-02, -8.2309e-03, -1.0467e-02, -5.4482e-02, -9.5878e-03,\n",
      "         -4.7281e-02,  2.0789e-02,  8.9954e-02, -8.3861e-02,  1.6708e-02,\n",
      "         -3.6441e-02, -1.1991e-02, -5.7313e-02,  2.9761e-02,  7.8992e-03,\n",
      "         -3.6963e-02,  1.1660e-02, -5.4706e-02,  4.6702e-02,  3.7148e-03,\n",
      "          6.3400e-04, -9.1871e-02,  2.5330e-03, -6.9191e-03,  3.6635e-02,\n",
      "         -5.2922e-02,  5.2562e-02, -6.3750e-02,  7.0911e-02,  3.1724e-02,\n",
      "          4.9951e-03,  4.0879e-02,  1.5243e-03,  2.5139e-02,  3.3294e-02,\n",
      "         -5.9938e-02, -4.2411e-02,  3.3242e-02, -5.3652e-04, -5.1480e-02,\n",
      "         -2.2070e-02,  8.5234e-03,  7.3571e-03,  5.4721e-02,  5.1512e-02,\n",
      "         -6.8792e-02, -2.9043e-02, -1.7342e-02, -4.2614e-02,  2.1688e-02,\n",
      "          2.6404e-02, -9.6383e-04, -4.2691e-02, -1.2735e-02,  1.7152e-02,\n",
      "          3.3568e-02, -1.9829e-02, -6.6677e-02, -1.2195e-02,  8.1720e-02,\n",
      "         -3.6547e-02,  4.0922e-02,  6.8748e-02, -1.5848e-02,  1.6116e-02,\n",
      "          5.3184e-02, -9.4376e-03,  5.5248e-02, -3.2326e-02, -8.7455e-03,\n",
      "         -5.6876e-02, -5.7324e-03,  8.3665e-02,  1.4265e-02, -1.2587e-02,\n",
      "         -1.0551e-02, -9.9775e-02,  4.8242e-02,  7.8034e-02,  1.6202e-02,\n",
      "          4.7069e-02,  8.3134e-03,  1.7954e-03, -4.2452e-02, -2.1928e-02,\n",
      "          1.9161e-02,  6.2105e-03,  1.8270e-02, -1.9072e-02, -6.8034e-02,\n",
      "          3.6826e-02, -1.0864e-02, -1.7525e-02, -2.9445e-02,  1.0102e-01,\n",
      "         -5.8730e-02, -6.8523e-02, -4.1363e-02,  4.6536e-02,  1.0418e-02,\n",
      "          2.7773e-02, -4.7237e-02, -1.0723e-02, -4.7896e-02,  7.8646e-02,\n",
      "         -3.9359e-02,  2.4520e-02,  2.6620e-02,  3.2236e-02, -3.0868e-02,\n",
      "          3.9751e-02, -6.2320e-02, -6.4986e-02,  2.1959e-02, -2.3098e-02,\n",
      "          4.3383e-03,  3.9995e-02,  1.4020e-02,  6.7921e-03, -7.9298e-02,\n",
      "          5.4260e-02,  4.0702e-03, -1.6723e-02,  2.6026e-02,  3.4545e-03,\n",
      "          7.9468e-03, -2.1413e-03,  5.9186e-02,  5.5072e-02,  2.4460e-02,\n",
      "          7.0254e-02, -4.3641e-02,  6.9120e-02, -7.2662e-02, -1.9774e-02,\n",
      "          3.1153e-02, -6.2465e-02,  1.8319e-02, -2.1393e-02, -2.5058e-03,\n",
      "          1.1188e-02, -5.4352e-02,  2.3447e-02, -4.5775e-02,  7.8184e-03,\n",
      "         -8.4942e-03,  4.4819e-02, -1.0326e-01,  3.8825e-02,  3.4499e-02,\n",
      "         -1.2309e-02,  5.7603e-02, -1.0961e-01,  2.3648e-02, -4.7898e-02,\n",
      "         -3.3618e-02, -2.9353e-02,  5.5454e-02, -1.1795e-01, -1.1660e-02,\n",
      "         -7.4058e-02,  2.2532e-02, -6.4373e-02,  2.8355e-02,  2.3525e-02,\n",
      "         -5.1050e-02, -6.9225e-02,  2.3589e-02, -3.4069e-02,  1.7130e-02,\n",
      "          6.1683e-03,  4.6698e-03,  3.1385e-02, -1.2499e-02, -6.1407e-02,\n",
      "         -1.2630e-03, -1.1415e-01,  2.0936e-02, -5.0608e-02,  2.7706e-03,\n",
      "          3.1321e-02, -6.0005e-02, -7.5825e-02,  6.9285e-02,  1.4949e-03,\n",
      "         -3.0213e-02,  4.8912e-02,  2.2268e-02, -5.6563e-02, -5.1873e-02,\n",
      "          5.2022e-03,  1.8384e-02, -1.5283e-02,  3.9217e-02,  1.7968e-02,\n",
      "         -5.9231e-03,  7.4997e-03,  7.2202e-03, -3.5587e-02,  9.3937e-02,\n",
      "          5.4957e-02,  5.6426e-02, -6.5349e-02,  1.3197e-02, -1.4102e-02,\n",
      "         -2.9120e-02, -1.8918e-02,  5.8982e-02,  5.3326e-02, -1.8322e-02,\n",
      "         -1.5117e-02,  2.2364e-02, -2.7512e-02,  7.6425e-02, -8.0679e-03,\n",
      "         -6.2225e-02,  7.6757e-03,  4.5510e-02,  3.5129e-02, -6.0821e-02,\n",
      "         -4.6603e-02,  3.8096e-02, -1.2460e-02,  3.4564e-02,  2.4936e-02,\n",
      "         -2.2061e-02,  3.3201e-02, -8.0073e-02,  3.6098e-02, -3.5388e-03,\n",
      "         -6.9225e-02, -2.5121e-02,  4.4906e-02, -2.6779e-02,  4.3848e-02,\n",
      "         -5.2007e-02, -3.3439e-03,  2.6037e-02,  7.4658e-03, -5.6472e-03,\n",
      "         -8.3039e-02, -2.4014e-02, -6.2195e-03,  2.9450e-02, -1.4905e-02,\n",
      "          2.2908e-02, -2.1393e-02,  1.0241e-02,  7.3360e-03,  1.0190e-01,\n",
      "          2.9442e-02, -1.5514e-02,  3.9805e-02, -9.7446e-03, -6.9531e-03,\n",
      "          2.8055e-02,  2.0084e-02, -7.5856e-02,  5.2009e-02, -4.2801e-02,\n",
      "         -9.1915e-02, -3.1268e-02,  1.9026e-02,  1.2120e-02, -3.0270e-02,\n",
      "         -8.9535e-02, -1.6083e-02,  2.6649e-02, -2.7651e-02,  1.8772e-02,\n",
      "          5.1707e-02,  6.3682e-02,  6.9662e-02, -1.9067e-02,  5.7304e-02,\n",
      "          3.1024e-02, -3.7315e-02,  7.1894e-02,  5.8520e-03, -3.5590e-02,\n",
      "          6.5263e-03, -3.7181e-03,  3.2432e-02, -7.4316e-02, -8.0504e-02,\n",
      "         -5.5104e-03,  1.5641e-02,  5.3833e-03,  1.6913e-03,  4.6166e-02,\n",
      "          5.1178e-02,  6.5070e-02,  5.4918e-02,  6.9356e-02, -8.6152e-03,\n",
      "          1.1706e-02,  2.1389e-02, -5.0035e-02, -3.2440e-02,  2.4755e-02,\n",
      "         -6.6282e-02,  5.8403e-02, -6.7037e-02, -6.4013e-03, -5.3235e-02,\n",
      "          3.9257e-02, -2.8180e-02, -6.2199e-03, -7.9191e-02,  3.1176e-02,\n",
      "         -4.6673e-02,  1.5490e-02, -6.2970e-03,  3.1618e-02, -3.1305e-02,\n",
      "         -2.5003e-02,  9.1036e-04, -8.0617e-03, -5.9850e-02, -2.6155e-02,\n",
      "          6.1301e-04, -3.7640e-02,  4.6412e-02,  8.4113e-02,  1.2751e-03,\n",
      "         -9.4646e-02,  1.5296e-02,  5.5135e-02, -2.9847e-02,  6.2518e-02,\n",
      "          5.4611e-02,  7.8381e-02, -3.1838e-02,  3.9854e-03, -7.2575e-02,\n",
      "          2.7253e-02, -1.4156e-03, -2.0416e-02,  2.8605e-02, -1.1525e-01,\n",
      "          3.3226e-02, -4.7544e-02, -3.1123e-03,  4.2567e-02,  3.2474e-03,\n",
      "          3.4342e-02,  9.4606e-05, -2.2935e-02, -4.2337e-02, -4.1849e-02,\n",
      "         -1.8011e-02,  2.7728e-02, -4.5713e-02, -3.1321e-02,  3.3685e-02,\n",
      "         -2.1008e-02,  4.9062e-02, -1.3385e-02,  2.1900e-02,  1.3259e-02,\n",
      "         -2.0064e-02, -2.8395e-02,  3.0163e-02,  1.6817e-02, -7.3453e-02,\n",
      "         -1.1369e-02, -1.8509e-02, -4.7963e-02,  6.7612e-02,  2.2239e-02,\n",
      "          4.1169e-02, -1.0413e-01, -2.8432e-02,  2.8063e-02, -9.6625e-03,\n",
      "         -6.5333e-02,  3.1132e-02,  3.0314e-03,  5.2295e-02,  5.1817e-02,\n",
      "         -8.4207e-03, -3.5923e-02, -7.9107e-02,  2.3508e-02,  3.0520e-02,\n",
      "         -3.5801e-02,  5.7796e-04, -5.3692e-02,  5.5316e-03,  2.1822e-02,\n",
      "          5.4210e-02,  8.6603e-02, -1.3827e-02, -3.5024e-02, -3.7394e-02,\n",
      "         -5.7029e-03, -3.5120e-02, -9.9439e-02,  6.9728e-02,  2.2530e-02,\n",
      "         -4.5383e-02, -5.1588e-02,  6.5462e-02,  2.0998e-02, -1.8429e-02,\n",
      "         -4.8042e-02, -2.7237e-02,  4.1896e-02,  2.7694e-02, -2.0625e-02,\n",
      "          1.1551e-03,  6.0283e-03, -1.0398e-02,  5.7727e-02,  3.3575e-02,\n",
      "         -1.3834e-02,  3.7005e-03, -2.2593e-02, -4.6989e-02,  6.0363e-02,\n",
      "          5.5251e-02,  6.0577e-02, -7.4297e-02,  2.1913e-02, -5.3980e-02,\n",
      "          2.6166e-03, -7.6182e-02,  9.0397e-03, -8.1741e-03,  8.1354e-02,\n",
      "         -6.3384e-03, -5.4922e-02,  2.8885e-02,  1.9748e-02,  1.0692e-02,\n",
      "         -6.6295e-03,  3.6824e-02, -6.8765e-02, -1.5727e-02,  6.0870e-02,\n",
      "          9.6155e-03, -3.4638e-02, -1.1363e-01, -2.8232e-02,  3.0548e-02,\n",
      "         -4.3769e-02,  9.0580e-02,  3.0619e-02,  2.5847e-02, -5.2429e-03,\n",
      "          3.6678e-02, -4.7999e-03,  3.9304e-03,  1.0586e-02, -5.5936e-03,\n",
      "          2.6363e-02,  7.4653e-02, -7.5637e-02, -2.3597e-03, -6.3676e-02,\n",
      "         -7.1860e-02,  3.7300e-03, -1.0882e-01,  1.6708e-02,  5.4546e-02,\n",
      "         -1.6844e-02, -5.3290e-02,  1.7931e-02, -1.6294e-02,  1.4423e-02,\n",
      "          4.7060e-03, -4.0227e-02]]) torch.Size([1, 512])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T04:27:02.977063Z",
     "start_time": "2025-02-24T04:27:02.713252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize MTCNN (Multi-task Cascaded Convolutional Networks) for face detection.\n",
    "# MTCNN is used to detect and align faces in images.\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load a pre-trained Inception ResNet model trained on the VGGFace2 dataset.\n",
    "# This model extracts feature embeddings (numerical representations) for detected faces.\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Load two face images to be verified (replace with actual image paths if needed).\n",
    "# These images will be compared to determine similarity.\n",
    "img1 = Image.open('data/images/img_2.jpg')\n",
    "img2 = Image.open('data/images/img_4.jpg')\n",
    "\n",
    "# Detect faces in each image using MTCNN.\n",
    "# `mtcnn.detect` returns face bounding boxes and confidence scores (unused here).\n",
    "faces1, _ = mtcnn.detect(img1)\n",
    "faces2, _ = mtcnn.detect(img2)\n",
    "\n",
    "# Proceed only if faces are detected in both images.\n",
    "if faces1 is not None and faces2 is not None:\n",
    "    # Align and preprocess the detected faces for feature embedding extraction.\n",
    "    # The `mtcnn` call aligns the face to the required input format for the model.\n",
    "    aligned1 = mtcnn(img1)\n",
    "    aligned2 = mtcnn(img2)\n",
    "\n",
    "    # Extract feature embeddings for the aligned faces using the Inception ResNet model.\n",
    "    # Add a batch dimension using `.unsqueeze(0)` to process a single image.\n",
    "    # `.detach()` removes the computation graph to treat the result as raw data.\n",
    "    embeddings1 = resnet(aligned1.unsqueeze(0)).detach()\n",
    "    embeddings2 = resnet(aligned2.unsqueeze(0)).detach()\n",
    "\n",
    "    # Compute the Euclidean distance between the two embeddings.\n",
    "    # A smaller distance indicates higher similarity between the faces.\n",
    "    distance = torch.dist(embeddings1, embeddings2)\n",
    "    print(f\"Distance between embeddings: {distance:.4f}\")\n",
    "\n",
    "    # Compare the distance with a threshold (e.g., 1.0) to determine similarity.\n",
    "    # If the distance is below the threshold, the faces are considered similar.\n",
    "    if distance < 1.0:\n",
    "        print(\"The two images are similar.\")\n",
    "    else:\n",
    "        print(\"The two images are different.\")\n",
    "else:\n",
    "    # If no face is detected in one or both images, print an appropriate message.\n",
    "    print(\"Face(s) not detected in one or both images.\")\n"
   ],
   "id": "f0115b202f6b6de2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between embeddings: 0.8801\n",
      "The two images are similar.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T04:25:23.448397Z",
     "start_time": "2025-02-24T04:25:22.883819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize MTCNN (Multi-task Cascaded Convolutional Networks) for face detection.\n",
    "# This is used to detect faces in the given images.\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load a pre-trained Inception ResNet model trained on VGGFace2 dataset.\n",
    "# This model generates feature embeddings for detected faces.\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# List of image paths where each image contains one or more faces to be processed.\n",
    "img_paths = ['data/images/img_1.jpg', 'data/images/img_2.jpg', 'data/images/img_3.jpg', 'data/images/img_4.jpg',\n",
    "             'data/images/img_5.jpg']\n",
    "\n",
    "# Initialize an empty list to store face embeddings for all detected faces.\n",
    "embeddings = []\n",
    "\n",
    "# Loop through each image file in the list.\n",
    "for img_path in img_paths:\n",
    "    # Open the image using PIL (Python Imaging Library).\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # Detect faces in the image using MTCNN and return face bounding boxes.\n",
    "    faces, _ = mtcnn.detect(img)\n",
    "\n",
    "    # Proceed only if faces are detected in the image.\n",
    "    if faces is not None:\n",
    "        # Align and preprocess the detected faces for the Inception ResNet model.\n",
    "        aligned = mtcnn(img)\n",
    "\n",
    "        # Compute the face embeddings (feature vectors) for the aligned face(s).\n",
    "        # Detach the computation graph to save only the embeddings as raw data.\n",
    "        face_embeddings = resnet(aligned.unsqueeze(0)).detach()\n",
    "\n",
    "        # Convert the embeddings to a NumPy array and add it to the embeddings list.\n",
    "        embeddings.append(face_embeddings.numpy())\n",
    "\n",
    "# Combine all individual face embeddings into a single NumPy array.\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Perform clustering on the embeddings using DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\n",
    "# 'eps' is the maximum distance between two samples to be considered as in the same neighborhood.\n",
    "# 'min_samples' is the minimum number of samples in a neighborhood required to form a cluster.\n",
    "clustering = DBSCAN(eps=0.5, min_samples=2).fit(embeddings)\n"
   ],
   "id": "7da1d2e8f2a7865e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T04:25:27.517296Z",
     "start_time": "2025-02-24T04:25:27.514923Z"
    }
   },
   "cell_type": "code",
   "source": "clustering.labels_",
   "id": "e27046381ae30558",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  0, -1, -1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
