{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. First, let's start with the imports and configuration:\n",
   "id": "68988d4908f1c2c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:44:04.460631Z",
     "start_time": "2025-04-29T03:43:56.577403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import os\n",
    "import logging\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "# Setup logging and device\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    MAX_LENGTH = 10\n",
    "    MIN_WORD_FREQ = 3\n",
    "    VOCAB_SIZE = 5000\n",
    "    BATCH_SIZE = 64\n",
    "    EMBEDDING_DIM = 256\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 4\n",
    "    DROPOUT = 0.1\n",
    "    LEARNING_RATE = 0.0003\n",
    "    EPOCHS = 6 #20\n",
    "\n",
    "    # Paths\n",
    "    CORPUS_CONV = 'data/cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "    CORPUS_LINES = 'data/cornell movie-dialogs corpus/movie_lines.txt'\n",
    "    DELIMITER = ' +++$+++ '"
   ],
   "id": "58aac405bb83a97",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/livyatan13/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/livyatan13/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Text preprocessing and data handling:",
   "id": "6e668a9238a8ea09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:44:04.604361Z",
     "start_time": "2025-04-29T03:44:04.598937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = ['<pad>', '<unk>', '<start>', '<end>']\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced text preprocessing with better linguistic features\"\"\"\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode()\n",
    "\n",
    "        # Preserve important punctuation and add spaces\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\',.!?\\s]', ' ', text)\n",
    "        text = re.sub(r'([.,!?])', r' \\1 ', text)\n",
    "\n",
    "        # Handle contractions carefully\n",
    "        contractions = {\n",
    "            \"\\'s\": \" 's\", \"\\'re\": \" 're\", \"\\'t\": \" 't\",\n",
    "            \"\\'ll\": \" 'll\", \"\\'ve\": \" 've\", \"\\'m\": \" 'm\",\n",
    "            \"won't\": \"will not\", \"can't\": \"cannot\"\n",
    "        }\n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def filter_conversation(self, question: str, answer: str) -> bool:\n",
    "        \"\"\"Filter out low-quality conversation pairs\"\"\"\n",
    "        min_words, max_words = 2, Config.MAX_LENGTH\n",
    "\n",
    "        q_words = len(question.split())\n",
    "        a_words = len(answer.split())\n",
    "\n",
    "        if not (min_words <= q_words <= max_words and\n",
    "                min_words <= a_words <= max_words):\n",
    "            return False\n",
    "\n",
    "        generic_responses = {\n",
    "            'i don t know', 'i m not sure', 'what', 'ok', 'yes', 'no',\n",
    "            'i do not know', 'i dont know', 'idk', 'i have no idea'\n",
    "        }\n",
    "        if answer.strip().lower() in generic_responses:\n",
    "            return False\n",
    "\n",
    "        return True"
   ],
   "id": "27c3326f47305fcf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data loading and processing:",
   "id": "4f5c9ef02de0940d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:44:04.670403Z",
     "start_time": "2025-04-29T03:44:04.654782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.text_processor = TextProcessor()\n",
    "        self.word_map = {}\n",
    "        \n",
    "    def encode_sequence(self, tokens: List[str], max_len: int) -> List[int]:\n",
    "        \"\"\"Encode token sequence to indices\"\"\"\n",
    "        # Convert tokens to indices\n",
    "        encoded = [self.word_map.get(token, self.word_map['<unk>']) \n",
    "                  for token in tokens]\n",
    "        \n",
    "        # Truncate or pad as needed\n",
    "        if len(encoded) > max_len:\n",
    "            encoded = encoded[:max_len]\n",
    "        elif len(encoded) < max_len:\n",
    "            encoded.extend([self.word_map['<pad>']] * (max_len - len(encoded)))\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def prepare_data(self) -> Tuple[List[List[List[int]]], Dict[str, int]]:\n",
    "        \"\"\"Prepare and encode all data\"\"\"\n",
    "        # Load raw data\n",
    "        conversations, lines_dict = self.load_conversations()\n",
    "        pairs = self.extract_pairs(conversations, lines_dict)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        word_freq = Counter()\n",
    "        for pair in pairs:\n",
    "            word_freq.update(pair[0])\n",
    "            word_freq.update(pair[1])\n",
    "        \n",
    "        # Filter words and create word map\n",
    "        words = [w for w, freq in word_freq.items() \n",
    "                if freq >= self.config.MIN_WORD_FREQ]\n",
    "        words = words[:self.config.VOCAB_SIZE - 4]  # Reserve space for special tokens\n",
    "        \n",
    "        # Create word map with special tokens\n",
    "        special_tokens = ['<pad>', '<unk>', '<start>', '<end>']\n",
    "        self.word_map = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "        for word in words:\n",
    "            if word not in self.word_map:\n",
    "                self.word_map[word] = len(self.word_map)\n",
    "        \n",
    "        # Encode all pairs\n",
    "        encoded_pairs = []\n",
    "        for pair in pairs:\n",
    "            encoded_q = self.encode_sequence(pair[0], self.config.MAX_LENGTH)\n",
    "            encoded_a = self.encode_sequence(pair[1], self.config.MAX_LENGTH)\n",
    "            encoded_pairs.append([encoded_q, encoded_a])\n",
    "        \n",
    "        return encoded_pairs, self.word_map\n",
    "\n",
    "    def load_conversations(self) -> Tuple[List[str], Dict[str, str]]:\n",
    "        \"\"\"Load conversation files with error handling\"\"\"\n",
    "        try:\n",
    "            with open(self.config.CORPUS_CONV, 'r', encoding='iso-8859-1') as f:\n",
    "                conversations = f.readlines()\n",
    "            with open(self.config.CORPUS_LINES, 'r', encoding='iso-8859-1') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            lines_dict = {}\n",
    "            for line in lines:\n",
    "                parts = line.split(self.config.DELIMITER)\n",
    "                if len(parts) >= 2:\n",
    "                    lines_dict[parts[0]] = parts[-1].strip()\n",
    "\n",
    "            return conversations, lines_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading files: {e}\")\n",
    "            return [], {}\n",
    "\n",
    "    def extract_pairs(self, conversations: List[str],\n",
    "                     lines_dict: Dict[str, str]) -> List[List[str]]:\n",
    "        \"\"\"Extract and filter conversation pairs\"\"\"\n",
    "        pairs = []\n",
    "        for conv in conversations:\n",
    "            try:\n",
    "                ids = eval(conv.split(self.config.DELIMITER)[-1])\n",
    "                for i in range(len(ids) - 1):\n",
    "                    q = lines_dict.get(ids[i], '')\n",
    "                    a = lines_dict.get(ids[i + 1], '')\n",
    "\n",
    "                    if q and a:\n",
    "                        q = self.text_processor.preprocess_text(q)\n",
    "                        a = self.text_processor.preprocess_text(a)\n",
    "\n",
    "                        if self.text_processor.filter_conversation(q, a):\n",
    "                            q_tokens = word_tokenize(q)[:self.config.MAX_LENGTH]\n",
    "                            a_tokens = word_tokenize(a)[:self.config.MAX_LENGTH]\n",
    "                            pairs.append([q_tokens, a_tokens])\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing conversation: {e}\")\n",
    "                continue\n",
    "\n",
    "        return pairs"
   ],
   "id": "e45da4f75a093220",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model architecture:\n",
   "id": "f909f4dbe442788e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:44:04.722369Z",
     "start_time": "2025-04-29T03:44:04.712105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_length: int = 100):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerChatbot(nn.Module):\n",
    "    def __init__(self, config: Config, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embeddings and positional encoding\n",
    "        self.embedding = nn.Embedding(vocab_size, config.EMBEDDING_DIM)\n",
    "        self.pos_encoder = PositionalEncoding(config.EMBEDDING_DIM)\n",
    "        self.dropout = nn.Dropout(config.DROPOUT)\n",
    "\n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.EMBEDDING_DIM,\n",
    "            nhead=config.NUM_HEADS,\n",
    "            dim_feedforward=config.EMBEDDING_DIM * 4,\n",
    "            dropout=config.DROPOUT,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=config.NUM_LAYERS\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(config.EMBEDDING_DIM, vocab_size)\n",
    "\n",
    "    def create_mask(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        src_mask = src == 0  # PAD token\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        # Create mask for padding tokens\n",
    "        src_mask = self.create_mask(src)\n",
    "\n",
    "        # Embedding and position encoding\n",
    "        src = self.embedding(src) * math.sqrt(self.config.EMBEDDING_DIM)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = self.dropout(src)\n",
    "\n",
    "        # Apply transformer with masking\n",
    "        output = self.transformer(src, src_key_padding_mask=src_mask)\n",
    "\n",
    "        # Generate output distribution\n",
    "        output = self.output_layer(output)\n",
    "\n",
    "        return output"
   ],
   "id": "ce1e38eb5df5c4b4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training components:\n",
   "id": "174d7bcc1c4622b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:44:04.779127Z",
     "start_time": "2025-04-29T03:44:04.769588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, pairs_encoded: List[List[List[int]]]):\n",
    "        self.pairs = pairs_encoded\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (torch.LongTensor(self.pairs[idx][0]),\n",
    "                torch.LongTensor(self.pairs[idx][1]))\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, config: Config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config.LEARNING_RATE\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(src)\n",
    "\n",
    "            loss = self.criterion(\n",
    "                output.view(-1, output.size(-1)),\n",
    "                tgt.view(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        return total_loss / len(train_loader)"
   ],
   "id": "35e24ee28576b4e2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chat interface and inference:\n",
   "id": "4c972aa621763570"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:44:04.833762Z",
     "start_time": "2025-04-29T03:44:04.825838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ChatInterface:\n",
    "    def __init__(self, model: TransformerChatbot,\n",
    "                 word_map: Dict[str, int],\n",
    "                 config: Config):\n",
    "        self.model = model\n",
    "        self.word_map = word_map\n",
    "        self.config = config\n",
    "        self.text_processor = TextProcessor()\n",
    "        self.rev_word_map = {idx: word for word, idx in word_map.items()}\n",
    "\n",
    "    def generate_response(self, question: str) -> str:\n",
    "        self.model.eval()\n",
    "\n",
    "        # Preprocess question\n",
    "        question = self.text_processor.preprocess_text(question)\n",
    "        tokens = word_tokenize(question)[:self.config.MAX_LENGTH]\n",
    "\n",
    "        # Convert to indices\n",
    "        indices = [self.word_map.get(token, self.word_map['<unk>'])\n",
    "                  for token in tokens]\n",
    "        # Pad if necessary\n",
    "        if len(indices) < self.config.MAX_LENGTH:\n",
    "            indices.extend([self.word_map['<pad>']] *\n",
    "                         (self.config.MAX_LENGTH - len(indices)))\n",
    "\n",
    "        question_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = self.model(question_tensor)\n",
    "            _, predicted = torch.max(output, dim=-1)\n",
    "\n",
    "        # Convert to words and clean up response\n",
    "        response_words = []\n",
    "        for idx in predicted[0]:\n",
    "            word = self.rev_word_map[idx.item()]\n",
    "            if word in ['<pad>', '<start>', '<end>', '<unk>']:\n",
    "                continue\n",
    "            response_words.append(word)\n",
    "\n",
    "        return ' '.join(response_words)"
   ],
   "id": "c905c2e4c50eb421",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finally, here's how to put it all together:",
   "id": "d913fb26901304d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T03:51:06.711336Z",
     "start_time": "2025-04-29T03:44:04.886393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Initialize data handler and prepare data\n",
    "data_handler = DataHandler(config)\n",
    "encoded_pairs, word_map = data_handler.prepare_data()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ChatDataset(encoded_pairs)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "model = TransformerChatbot(config, len(word_map)).to(device)\n",
    "trainer = Trainer(model, config)\n",
    "\n",
    "# Train the model\n",
    "logger.info(\"Starting training...\")\n",
    "for epoch in range(config.EPOCHS):\n",
    "    avg_loss = trainer.train_epoch(train_loader)\n",
    "    logger.info(f\"Epoch {epoch} completed, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Create chat interface\n",
    "chat_interface = ChatInterface(model, word_map, config)\n",
    "\n",
    "# Test the chatbot\n",
    "test_questions = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What's your favorite movie?\",\n",
    "    \"Do you like pizza?\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the chatbot:\")\n",
    "for question in test_questions:\n",
    "    response = chat_interface.generate_response(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response}\\n\")\n"
   ],
   "id": "8da6761c8a4c71ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting training...\n",
      "INFO:__main__:Batch 0, Loss: 8.5772\n",
      "INFO:__main__:Batch 100, Loss: 5.1684\n",
      "INFO:__main__:Batch 200, Loss: 5.2472\n",
      "INFO:__main__:Batch 300, Loss: 5.3433\n",
      "INFO:__main__:Batch 400, Loss: 5.0134\n",
      "INFO:__main__:Batch 500, Loss: 5.2570\n",
      "INFO:__main__:Batch 600, Loss: 5.0929\n",
      "INFO:__main__:Batch 700, Loss: 5.1147\n",
      "INFO:__main__:Batch 800, Loss: 4.8993\n",
      "INFO:__main__:Batch 900, Loss: 4.6280\n",
      "INFO:__main__:Batch 1000, Loss: 4.9032\n",
      "INFO:__main__:Epoch 0 completed, Average Loss: 5.1118\n",
      "INFO:__main__:Batch 0, Loss: 4.8707\n",
      "INFO:__main__:Batch 100, Loss: 5.0789\n",
      "INFO:__main__:Batch 200, Loss: 5.0667\n",
      "INFO:__main__:Batch 300, Loss: 5.1085\n",
      "INFO:__main__:Batch 400, Loss: 4.8963\n",
      "INFO:__main__:Batch 500, Loss: 5.0920\n",
      "INFO:__main__:Batch 600, Loss: 4.7637\n",
      "INFO:__main__:Batch 700, Loss: 5.0541\n",
      "INFO:__main__:Batch 800, Loss: 4.8991\n",
      "INFO:__main__:Batch 900, Loss: 5.1809\n",
      "INFO:__main__:Batch 1000, Loss: 4.9116\n",
      "INFO:__main__:Epoch 1 completed, Average Loss: 4.9755\n",
      "INFO:__main__:Batch 0, Loss: 4.7429\n",
      "INFO:__main__:Batch 100, Loss: 4.8812\n",
      "INFO:__main__:Batch 200, Loss: 4.8195\n",
      "INFO:__main__:Batch 300, Loss: 4.8707\n",
      "INFO:__main__:Batch 400, Loss: 4.7912\n",
      "INFO:__main__:Batch 500, Loss: 5.0117\n",
      "INFO:__main__:Batch 600, Loss: 5.1265\n",
      "INFO:__main__:Batch 700, Loss: 4.9849\n",
      "INFO:__main__:Batch 800, Loss: 4.7474\n",
      "INFO:__main__:Batch 900, Loss: 4.9153\n",
      "INFO:__main__:Batch 1000, Loss: 5.1111\n",
      "INFO:__main__:Epoch 2 completed, Average Loss: 4.9214\n",
      "INFO:__main__:Batch 0, Loss: 4.7402\n",
      "INFO:__main__:Batch 100, Loss: 4.8689\n",
      "INFO:__main__:Batch 200, Loss: 4.6564\n",
      "INFO:__main__:Batch 300, Loss: 5.0342\n",
      "INFO:__main__:Batch 400, Loss: 4.8166\n",
      "INFO:__main__:Batch 500, Loss: 4.4766\n",
      "INFO:__main__:Batch 600, Loss: 4.9354\n",
      "INFO:__main__:Batch 700, Loss: 4.7915\n",
      "INFO:__main__:Batch 800, Loss: 5.0093\n",
      "INFO:__main__:Batch 900, Loss: 4.9042\n",
      "INFO:__main__:Batch 1000, Loss: 4.8823\n",
      "INFO:__main__:Epoch 3 completed, Average Loss: 4.8697\n",
      "INFO:__main__:Batch 0, Loss: 4.9262\n",
      "INFO:__main__:Batch 100, Loss: 4.8208\n",
      "INFO:__main__:Batch 200, Loss: 4.7681\n",
      "INFO:__main__:Batch 300, Loss: 4.7917\n",
      "INFO:__main__:Batch 400, Loss: 4.7758\n",
      "INFO:__main__:Batch 500, Loss: 4.8486\n",
      "INFO:__main__:Batch 600, Loss: 4.7024\n",
      "INFO:__main__:Batch 700, Loss: 4.9129\n",
      "INFO:__main__:Batch 800, Loss: 4.8214\n",
      "INFO:__main__:Batch 900, Loss: 4.8733\n",
      "INFO:__main__:Batch 1000, Loss: 4.7901\n",
      "INFO:__main__:Epoch 4 completed, Average Loss: 4.8150\n",
      "INFO:__main__:Batch 0, Loss: 4.6352\n",
      "INFO:__main__:Batch 100, Loss: 4.8591\n",
      "INFO:__main__:Batch 200, Loss: 4.6991\n",
      "INFO:__main__:Batch 300, Loss: 4.6733\n",
      "INFO:__main__:Batch 400, Loss: 4.6905\n",
      "INFO:__main__:Batch 500, Loss: 4.8741\n",
      "INFO:__main__:Batch 600, Loss: 4.7143\n",
      "INFO:__main__:Batch 700, Loss: 4.7262\n",
      "INFO:__main__:Batch 800, Loss: 4.7097\n",
      "INFO:__main__:Batch 900, Loss: 4.8284\n",
      "INFO:__main__:Batch 1000, Loss: 4.8237\n",
      "INFO:__main__:Epoch 5 completed, Average Loss: 4.7577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the chatbot:\n",
      "Q: Hello, how are you?\n",
      "A: i . . . . . to to to to\n",
      "\n",
      "Q: What's your favorite movie?\n",
      "A: . . . . . to to to to\n",
      "\n",
      "Q: Do you like pizza?\n",
      "A: i . . . . to to to to to\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
