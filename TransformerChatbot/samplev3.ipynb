{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chatbot - Transformer",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:47:42.294389Z",
     "start_time": "2025-04-27T18:47:41.025266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('punkt_tab')\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leviathan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/leviathan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:47:42.339520Z",
     "start_time": "2025-04-27T18:47:42.337877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('data/cornell movie-dialogs corpus'):\n",
    "    !wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip -P data && cd data && unzip cornell_movie_dialogs_corpus.zip\n",
    "else:\n",
    "    print(\"'data/cornell movie-dialogs corpus' already exists. Skipping download and extraction.\")"
   ],
   "id": "802a115419cba78e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/cornell movie-dialogs corpus' already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Preprocessing Part 1",
   "id": "e8b1a71858ec98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:48:10.059402Z",
     "start_time": "2025-04-27T18:47:42.383664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_movie_conv = 'data/cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "corpus_movie_lines = 'data/cornell movie-dialogs corpus/movie_lines.txt'\n",
    "\n",
    "max_len = 13 #\n",
    "\n",
    "import chardet\n",
    "\n",
    "\n",
    "def read_file_with_detected_encoding(file_path):\n",
    "    # Detect the file's encoding\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    # Fallback to ISO-8859-1 if encoding is not detected or is None\n",
    "    if not detected_encoding:\n",
    "        print(f\"Encoding detection failed for {file_path}. Falling back to ISO-8859-1.\")\n",
    "        detected_encoding = 'ISO-8859-1'\n",
    "\n",
    "    # Read the file using the detected encoding\n",
    "    with open(file_path, 'r', encoding=detected_encoding) as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "# Read files with dynamic encoding detection\n",
    "conv = read_file_with_detected_encoding(corpus_movie_conv)\n",
    "lines = read_file_with_detected_encoding(corpus_movie_lines)\n",
    "\n",
    "# Output summary (optional)\n",
    "print(f\"Successfully read {len(conv)} lines from {corpus_movie_conv}\")\n",
    "print(f\"Successfully read {len(lines)} lines from {corpus_movie_lines}\")\n",
    "\n",
    "DELIMITER = ' +++$+++ '  # Extract delimiter as a reusable constant\n",
    "\n",
    "\n",
    "def parse_lines_to_dict(lines):\n",
    "    \"\"\"Parse a list of lines into a dictionary with the first item as the key and the last as the value.\"\"\"\n",
    "    return {line.split(DELIMITER)[0]: line.split(DELIMITER)[-1] for line in lines}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "lines_dict = parse_lines_to_dict(lines)\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "\n",
    "def remove_punctuation(text, to_lowercase=True):\n",
    "    \"\"\"Remove punctuation from a string, including Unicode punctuation.\"\"\"\n",
    "    result = ''.join(\n",
    "        char for char in text\n",
    "        if char not in string.punctuation and\n",
    "        not unicodedata.category(char).startswith('P')  # 'P' stands for punctuation\n",
    "    )\n",
    "    return result.lower() if to_lowercase else result\n",
    "\n",
    "import ast  # To safely parse strings into Python objects\n",
    "\n",
    "\n",
    "def generate_qa_pairs(conversation, line_dict, delimiter, max_length):\n",
    "    \"\"\"Generate QA pairs (question-answer pairs) from a single conversation.\"\"\"\n",
    "\n",
    "    # Parse the list of line IDs from the conversation string using the delimiter\n",
    "    ids = ast.literal_eval(conversation.split(delimiter)[-1])  # Safely evaluate the IDs as a Python list\n",
    "    last_index = len(ids) - 1  # Find the index of the last ID in the conversation\n",
    "    qa_pairs = []  # Initialize an empty list to hold the QA pairs\n",
    "\n",
    "    for i, current_id in enumerate(ids):  # Iterate through each ID in the list along with its index\n",
    "        if i == last_index:  # If it's the last ID in the list, stop the loop as there is no next sentence\n",
    "            break\n",
    "\n",
    "        # Retrieve and clean the first sentence (current ID's line), trimming it to the max length\n",
    "        first_sentence = remove_punctuation(line_dict[current_id].strip()).split()[:max_length]\n",
    "\n",
    "        # Retrieve and clean the second sentence (next ID's line), trimming it to the max length\n",
    "        second_sentence = remove_punctuation(line_dict[ids[i + 1]].strip()).split()[:max_length]\n",
    "\n",
    "        # Append the pair of sentences as a QA pair to the list\n",
    "        qa_pairs.append([first_sentence, second_sentence])\n",
    "\n",
    "    # Return the final list of QA pairs generated from the conversation\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "# Main logic refactored with the function\n",
    "all_pairs = []  # Initialize an empty list to store all QA pairs from all conversations\n",
    "for conversation in conv:  # Iterate through each conversation in the conversations list\n",
    "    # Extend the 'all_pairs' list by adding QA pairs generated for the current conversation\n",
    "    all_pairs.extend(generate_qa_pairs(conversation, lines_dict, DELIMITER, max_len))\n",
    "\n",
    "# 'all_pairs' now contains all the QA pairs generated from all conversations\n",
    "\n",
    "\n",
    "word_frequency = Counter()\n",
    "for pair in all_pairs:\n",
    "    word_frequency.update(pair[0])\n",
    "    word_frequency.update(pair[1])\n",
    "\n",
    "\n",
    "# Constants\n",
    "MIN_WORD_FREQUENCY = 5\n",
    "SPECIAL_TOKENS = ['<unk>', '<start>', '<end>', '<pad>']\n",
    "\n",
    "\n",
    "# Function to create the word map\n",
    "def create_word_map(word_frequency, min_frequency, special_tokens):\n",
    "    words = [word for word in word_frequency.keys() if word_frequency[word] > min_frequency]\n",
    "    word_map = {word: index for index, word in enumerate(words, start=1)}  # Start indexing from 1\n",
    "\n",
    "    # Add special tokens\n",
    "    next_index = len(word_map) + 1\n",
    "    for token in special_tokens:\n",
    "        word_map[token] = next_index if token != '<pad>' else 0\n",
    "        if token != '<pad>':  # Don't increment index for '<pad>'\n",
    "            next_index += 1\n",
    "\n",
    "    return word_map\n",
    "\n",
    "\n",
    "# Usage example\n",
    "word_map = create_word_map(word_frequency, MIN_WORD_FREQUENCY, SPECIAL_TOKENS)\n",
    "\n",
    "\n",
    "with open('data/word_map_corpus.json', 'w') as f:\n",
    "    json.dump(word_map, f)\n",
    "\n",
    "class Encoder:\n",
    "    # Constants for special tokens\n",
    "    START_TOKEN = '<start>'\n",
    "    END_TOKEN = '<end>'\n",
    "    PAD_TOKEN = '<pad>'\n",
    "    UNKNOWN_TOKEN = '<unk>'\n",
    "\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "\n",
    "    def _pad_sequence(self, sequence, max_length):\n",
    "        \"\"\"Add padding tokens to reach desired length\"\"\"\n",
    "        padding_length = max_length - len(sequence)\n",
    "        return sequence + [self.word_map[self.PAD_TOKEN]] * padding_length\n",
    "\n",
    "    def encode_question(self, words, max_length):\n",
    "        \"\"\"Encode question sequence with padding\"\"\"\n",
    "        encoded = [self.word_map.get(word, self.word_map[self.UNKNOWN_TOKEN])\n",
    "                   for word in words]\n",
    "        return self._pad_sequence(encoded, max_length)\n",
    "\n",
    "    def encode_reply(self, words, max_length):\n",
    "        \"\"\"Encode reply sequence with start/end tokens and padding\"\"\"\n",
    "        # Reserve space for start and end tokens by reducing max_length by 2\n",
    "        words = words[:max_length - 2]  # Truncate words to leave room for special tokens\n",
    "        encoded = ([self.word_map[self.START_TOKEN]] +\n",
    "                   [self.word_map.get(word, self.word_map[self.UNKNOWN_TOKEN])\n",
    "                    for word in words] +\n",
    "                   [self.word_map[self.END_TOKEN]])\n",
    "        return self._pad_sequence(encoded, max_length)\n",
    "\n",
    "\n",
    "encoder = Encoder(word_map)\n",
    "question_encoded = encoder.encode_question(all_pairs[0][0], max_len)\n",
    "question_encoded\n",
    "\n",
    "pairs_encoded = []\n",
    "for pair in all_pairs:\n",
    "    question_encoded = encoder.encode_question(pair[0], max_len)\n",
    "    reply_encoded = encoder.encode_reply(pair[1], max_len)\n",
    "    pairs_encoded.append([question_encoded, reply_encoded])\n",
    "\n",
    "with open('data/pairs_encoded.json', 'w') as f:\n",
    "    json.dump(pairs_encoded, f)\n",
    "\n",
    "    # Check maximum lengths in pairs_encoded\n",
    "max_question_length = 0\n",
    "max_reply_length = 0\n",
    "\n",
    "for pair in pairs_encoded:\n",
    "    question, reply = pair\n",
    "    max_question_length = max(max_question_length, len(question))\n",
    "    max_reply_length = max(max_reply_length, len(reply))\n",
    "\n",
    "print(f\"Maximum question length: {max_question_length}\")\n",
    "print(f\"Maximum reply length: {max_reply_length}\")\n",
    "\n",
    "# Also check if any sequence exceeds the max_len\n",
    "exceeds_limit = False\n",
    "for pair in pairs_encoded:\n",
    "    question, reply = pair\n",
    "    if len(question) > max_len or len(reply) > max_len:\n",
    "        exceeds_limit = True\n",
    "        break\n",
    "\n",
    "print(f\"\\nAre there any sequences exceeding max_len ({max_len})? {exceeds_limit}\")\n"
   ],
   "id": "e9a11e86559684e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 83097 lines from data/cornell movie-dialogs corpus/movie_conversations.txt\n",
      "Successfully read 304713 lines from data/cornell movie-dialogs corpus/movie_lines.txt\n",
      "Maximum question length: 13\n",
      "Maximum reply length: 13\n",
      "\n",
      "Are there any sequences exceeding max_len (13)? False\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:04.733057Z",
     "start_time": "2025-04-27T19:00:51.602067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "nltk.download('punkt')\n",
    "\n",
    "# File paths\n",
    "CORPUS_CONV = 'data/cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "CORPUS_LINES = 'data/cornell movie-dialogs corpus/movie_lines.txt'\n",
    "max_len = 13\n",
    "DELIMITER = ' +++$+++ '\n",
    "\n",
    "def load_conversations():\n",
    "    \"\"\"Load and parse conversation files with error handling\"\"\"\n",
    "    try:\n",
    "        with open(CORPUS_CONV, 'r', encoding='iso-8859-1') as f:\n",
    "            conversations = f.readlines()\n",
    "        with open(CORPUS_LINES, 'r', encoding='iso-8859-1') as f:\n",
    "            lines = f.readlines()\n",
    "        return conversations, lines\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_lines_dict(lines):\n",
    "    \"\"\"Create dictionary of line ID to text with better parsing\"\"\"\n",
    "    lines_dict = {}\n",
    "    for line in lines:\n",
    "        parts = line.split(DELIMITER)\n",
    "        if len(parts) >= 2:\n",
    "            lines_dict[parts[0]] = parts[-1].strip()\n",
    "    return lines_dict\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Improved text preprocessing\"\"\"\n",
    "    # Convert to lowercase and normalize unicode\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode()\n",
    "\n",
    "    # Remove special characters but keep apostrophes for contractions\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\'\\s]', ' ', text)\n",
    "\n",
    "    # Standardize contractions\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def extract_pairs(conversations, lines_dict):\n",
    "    \"\"\"Extract and preprocess conversation pairs\"\"\"\n",
    "    pairs = []\n",
    "    for conv in conversations:\n",
    "        try:\n",
    "            ids = eval(conv.split(DELIMITER)[-1])\n",
    "            for i in range(len(ids) - 1):\n",
    "                first = lines_dict.get(ids[i], '')\n",
    "                second = lines_dict.get(ids[i + 1], '')\n",
    "\n",
    "                if first and second:\n",
    "                    first = preprocess_text(first)\n",
    "                    second = preprocess_text(second)\n",
    "\n",
    "                    # Add debug prints to see the text before tokenization\n",
    "                    # print(f\"Pre-tokenization first: '{first}'\")\n",
    "                    # print(f\"Pre-tokenization second: '{second}'\")\n",
    "\n",
    "                    try:\n",
    "                        first_tokens = word_tokenize(first)[:max_len]\n",
    "                        second_tokens = word_tokenize(second)[:max_len]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Tokenization error: {e}\")\n",
    "                        continue\n",
    "                    #\n",
    "                    # print(f'Tokens first: {first_tokens}')\n",
    "                    # print(f'Tokens second: {second_tokens}')\n",
    "\n",
    "                    if first_tokens and second_tokens:  # Only add if both have content\n",
    "                        pairs.append([first_tokens, second_tokens])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing conversation: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Total pairs extracted: {len(pairs)}\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def build_vocab(pairs, min_freq=1):\n",
    "    \"\"\"Build vocabulary with proper ID allocation\"\"\"\n",
    "    word_freq = Counter()\n",
    "    for pair in pairs:\n",
    "        word_freq.update(pair[0])\n",
    "        word_freq.update(pair[1])\n",
    "\n",
    "    # Filter out rare words\n",
    "    words = [w for w, freq in word_freq.items() if freq >= min_freq]\n",
    "\n",
    "    # Start with special tokens to ensure they have predictable IDs\n",
    "    special_tokens = ['<pad>', '<unk>', '<start>', '<end>']\n",
    "\n",
    "    # Create word map with continuous IDs\n",
    "    word_map = {}\n",
    "    # First add special tokens\n",
    "    for i, token in enumerate(special_tokens):\n",
    "        word_map[token] = i\n",
    "\n",
    "    # Then add all other words\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_map:  # Skip if it's somehow a special token\n",
    "            word_map[word] = i + len(special_tokens)\n",
    "\n",
    "    print(f\"Final vocabulary size: {len(word_map)}\")\n",
    "    return word_map\n",
    "\n",
    "def encode_sequence(tokens, word_map, is_reply=False, max_seq_len=5):\n",
    "    \"\"\"Encode sequence with correct length handling\"\"\"\n",
    "    if is_reply:\n",
    "        # For replies, we need to account for <start> and <end> tokens\n",
    "        # So actual token count will be limited to max_seq_len-2\n",
    "        tokens = tokens[:max_seq_len-2]  # Limit tokens to leave room for special tokens\n",
    "\n",
    "        # Add start token, tokens, and end token\n",
    "        encoded = [word_map['<start>']]\n",
    "        encoded.extend([word_map.get(token, word_map['<unk>']) for token in tokens])\n",
    "        encoded.append(word_map['<end>'])\n",
    "    else:\n",
    "        # For questions, we can use all max_seq_len positions\n",
    "        tokens = tokens[:max_seq_len]  # Limit tokens to max_seq_len\n",
    "        encoded = [word_map.get(token, word_map['<unk>']) for token in tokens]\n",
    "\n",
    "    # Calculate correct padding length\n",
    "    padding_length = max_seq_len - len(encoded)\n",
    "\n",
    "    # Add padding if needed\n",
    "    if padding_length > 0:\n",
    "        encoded.extend([word_map['<pad>']] * padding_length)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "# Load and process data\n",
    "conversations, lines = load_conversations()\n",
    "lines_dict = create_lines_dict(lines)\n",
    "pairs = extract_pairs(conversations, lines_dict)\n",
    "word_map = build_vocab(pairs)\n",
    "\n",
    "# Encode pairs with fixed function\n",
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    question = encode_sequence(pair[0], word_map, is_reply=False, max_seq_len=max_len)\n",
    "    answer = encode_sequence(pair[1], word_map, is_reply=True, max_seq_len=max_len)\n",
    "\n",
    "    # Validate lengths (important debugging check)\n",
    "    if len(question) != max_len or len(answer) != max_len:\n",
    "        print(f\"Warning: Inconsistent sequence length: Q:{len(question)}, A:{len(answer)}\")\n",
    "        continue\n",
    "\n",
    "    pairs_encoded.append([question, answer])\n",
    "\n",
    "\n",
    "# Add this after preprocessing a few examples\n",
    "sample_pairs = pairs[:5]\n",
    "print(\"\\nSample processed pairs:\")\n",
    "for pair in sample_pairs:\n",
    "    print(f\"Q: {' '.join(pair[0])}\")\n",
    "    print(f\"A: {' '.join(pair[1])}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Save processed data\n",
    "with open('data/word_map_corpus.json', 'w') as f:\n",
    "    json.dump(word_map, f)\n",
    "with open('data/pairs_encoded.json', 'w') as f:\n",
    "    json.dump(pairs_encoded, f)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_map)}\")\n",
    "print(f\"Number of conversation pairs: {len(pairs_encoded)}\")\n",
    "\n",
    "max_question_length = 0\n",
    "max_reply_length = 0\n",
    "\n",
    "for pair in pairs_encoded:\n",
    "    question, reply = pair\n",
    "    max_question_length = max(max_question_length, len(question))\n",
    "    max_reply_length = max(max_reply_length, len(reply))\n",
    "\n",
    "print(f\"Maximum question length: {max_question_length}\")\n",
    "print(f\"Maximum reply length: {max_reply_length}\")\n",
    "\n",
    "# Also check if any sequence exceeds the max_len\n",
    "exceeds_limit = False\n",
    "for pair in pairs_encoded:\n",
    "    question, reply = pair\n",
    "    if len(question) > max_len or len(reply) > max_len:\n",
    "        exceeds_limit = True\n",
    "        break\n",
    "\n",
    "print(f\"\\nAre there any sequences exceeding max_len ({max_len})? {exceeds_limit}\")"
   ],
   "id": "6d077314f17e5655",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leviathan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs extracted: 221274\n",
      "Final vocabulary size: 41506\n",
      "\n",
      "Sample processed pairs:\n",
      "Q: can we make this quick roxanne korrine and andrew barrett are having an\n",
      "A: well i thought we 'd start with pronunciation if that is okay with\n",
      "\n",
      "Q: well i thought we 'd start with pronunciation if that is okay with\n",
      "A: not the hacking and gagging and spitting part please\n",
      "\n",
      "Q: not the hacking and gagging and spitting part please\n",
      "A: okay then how 'bout we try out some french cuisine saturday night\n",
      "\n",
      "Q: you are asking me out that is so cute what is your name\n",
      "A: forget it\n",
      "\n",
      "Q: no no it is my fault we didn not have a proper introduction\n",
      "A: cameron\n",
      "\n",
      "Vocabulary size: 41506\n",
      "Number of conversation pairs: 221274\n",
      "Maximum question length: 13\n",
      "Maximum reply length: 13\n",
      "\n",
      "Are there any sequences exceeding max_len (13)? False\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading and Masking",
   "id": "5e37252872d6f0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:15.397556Z",
     "start_time": "2025-04-27T19:01:15.395032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ChatDataset(Dataset):\n",
    "    \"\"\"Dataset class for handling chat conversation pairs.\"\"\"\n",
    "\n",
    "    DEFAULT_DATA_PATH = 'data/pairs_encoded.json'\n",
    "\n",
    "    def __init__(self, data_path=DEFAULT_DATA_PATH):\n",
    "        \"\"\"Initialize dataset with conversation pairs.\n",
    "\n",
    "        Args:\n",
    "            data_path: Path to the JSON file containing encoded pairs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(data_path, 'r') as file:\n",
    "                self.pairs = json.load(file)\n",
    "            if not self.pairs:\n",
    "                raise ValueError(\"Empty dataset loaded\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Data file not found at: {data_path}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"Invalid JSON format in file: {data_path}\")\n",
    "\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get a conversation pair at the specified index.\"\"\"\n",
    "        pair = self.pairs[index]\n",
    "        question, reply = pair[0], pair[1]\n",
    "        return torch.LongTensor(question), torch.LongTensor(reply)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of conversation pairs.\"\"\"\n",
    "        return self.dataset_size"
   ],
   "id": "2c9105da109990eb",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:16.154545Z",
     "start_time": "2025-04-27T19:01:15.854323Z"
    }
   },
   "cell_type": "code",
   "source": "train_loader = DataLoader(ChatDataset(), batch_size=100, shuffle=True, pin_memory=True)",
   "id": "41f8ca6db0c3d13b",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:16.305227Z",
     "start_time": "2025-04-27T19:01:16.296277Z"
    }
   },
   "cell_type": "code",
   "source": "question, reply = next(iter(train_loader))",
   "id": "b84afd8740275375",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:16.549477Z",
     "start_time": "2025-04-27T19:01:16.547364Z"
    }
   },
   "cell_type": "code",
   "source": "question.shape",
   "id": "b1c4b014ffce2d2c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 13])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:17.385286Z",
     "start_time": "2025-04-27T19:01:17.383594Z"
    }
   },
   "cell_type": "code",
   "source": "reply.shape",
   "id": "251182c2dc9b001",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 13])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:17.433983Z",
     "start_time": "2025-04-27T19:01:17.432124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "\n",
    "    question_mask = (question != 0).to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    reply_input_mask = reply_input != 0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data)\n",
    "    # (batch_size, max_words, max_words)\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_target_mask = reply_target != 0\n",
    "\n",
    "    return question_mask, reply_input_mask, reply_target_mask\n"
   ],
   "id": "290a578217929c9c",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:17.521872Z",
     "start_time": "2025-04-27T19:01:17.519792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "size = 5\n",
    "print(torch.triu(torch.ones(size, size)).transpose(0, 1))"
   ],
   "id": "61040ac5bc20ba9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:17.609902Z",
     "start_time": "2025-04-27T19:01:17.607689Z"
    }
   },
   "cell_type": "code",
   "source": "question[0] != 0",
   "id": "a8a30a678312a5b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embeddings",
   "id": "c09d689e5a4e7f2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:21.057672Z",
     "start_time": "2025-04-27T19:01:21.054835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embeddings(nn.Module):\n",
    "    DEFAULT_MAX_LENGTH = 50\n",
    "    DROPOUT_RATE = 0.1\n",
    "    POSITION_ENCODING_BASE = 10000\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length=DEFAULT_MAX_LENGTH):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.dropout = nn.Dropout(self.DROPOUT_RATE)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Create position encoding on CPU and register as buffer\n",
    "        # Using register_buffer is crucial as it handles device transfers properly\n",
    "        position_encoding = self._create_position_encoding(max_length, embedding_dim)\n",
    "        self.register_buffer('position_encoding', position_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get sequence length from input\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Apply token embedding\n",
    "        embeddings = self.token_embedding(x) * math.sqrt(self.embedding_dim)\n",
    "\n",
    "        # Add positional encoding (slice to match sequence length)\n",
    "        embeddings = embeddings + self.position_encoding[:seq_len, :]\n",
    "\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "    def _create_position_encoding(self, max_length, embedding_dim):\n",
    "        \"\"\"Create position encoding matrix with sinusoidal patterns\"\"\"\n",
    "        position_encoding = torch.zeros(max_length, embedding_dim)\n",
    "\n",
    "        for pos in range(max_length):\n",
    "            for dim in range(0, embedding_dim, 2):\n",
    "                div_term = math.exp(dim * -math.log(self.POSITION_ENCODING_BASE) / embedding_dim)\n",
    "                position_encoding[pos, dim] = math.sin(pos * div_term)\n",
    "                if dim + 1 < embedding_dim:\n",
    "                    position_encoding[pos, dim + 1] = math.cos(pos * div_term)\n",
    "\n",
    "        return position_encoding"
   ],
   "id": "ab2f9dc2315e3b3c",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MultiHead Attention Implementation Part 1",
   "id": "d2fd1e3e611ec223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:24.255802Z",
     "start_time": "2025-04-27T19:01:24.252351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism implementation.\n",
    "\n",
    "    Allows the model to jointly attend to information from different representation\n",
    "    subspaces at different positions.\n",
    "    \"\"\"\n",
    "    DROPOUT_RATE = 0.1\n",
    "    ATTENTION_MASK_FILL_VALUE = -1e9\n",
    "\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        \"\"\"Initialize the multi-head attention layer.\n",
    "\n",
    "        Args:\n",
    "            num_heads: Number of attention heads\n",
    "            d_model: Dimension of the model\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self._validate_dimensions(d_model, num_heads)\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear layers for transformations\n",
    "        self.query_transform = nn.Linear(d_model, d_model)\n",
    "        self.key_transform = nn.Linear(d_model, d_model)\n",
    "        self.value_transform = nn.Linear(d_model, d_model)\n",
    "        self.output_transform = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.DROPOUT_RATE)\n",
    "\n",
    "    def _validate_dimensions(self, d_model, num_heads):\n",
    "        \"\"\"Validate that model dimensions are compatible with number of heads.\"\"\"\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by the number of heads\")\n",
    "\n",
    "    def _split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def _calculate_attention_scores(self, query, key):\n",
    "        \"\"\"Calculate raw attention scores.\"\"\"\n",
    "        return torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Compute multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "            key: Key tensor of shape (batch_size, seq_len, d_model)\n",
    "            value: Value tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Attention mask of shape (batch_size, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear transformations\n",
    "        query = self.query_transform(query)\n",
    "        key = self.key_transform(key)\n",
    "        value = self.value_transform(value)\n",
    "\n",
    "        # Split heads\n",
    "        query = self._split_heads(query, batch_size)\n",
    "        key = self._split_heads(key, batch_size)\n",
    "        value = self._split_heads(value, batch_size)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention_scores = self._calculate_attention_scores(query, key)\n",
    "\n",
    "        # Apply mask and softmax\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, self.ATTENTION_MASK_FILL_VALUE)\n",
    "        attention_weights = self.dropout(F.softmax(attention_scores, dim=-1))\n",
    "\n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        # Combine heads and apply final transformation\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.output_transform(output)"
   ],
   "id": "fc41107ddd6c6002",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:25.724218Z",
     "start_time": "2025-04-27T19:01:25.721624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward neural network module with two linear layers.\n",
    "\n",
    "    Implements a feed-forward network that consists of two linear transformations\n",
    "    with a ReLU activation and dropout in between.\n",
    "    \"\"\"\n",
    "\n",
    "    DROPOUT_RATE = 0.1\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=2048) -> None:\n",
    "        \"\"\"Initialize the feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimension of input features\n",
    "            hidden_dim: Dimension of hidden layer (default: 2048)\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(self.DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor\n",
    "\n",
    "        Returns:\n",
    "            Processed tensor after passing through the feed-forward layers\n",
    "        \"\"\"\n",
    "        hidden = F.relu(self.fc1(x))\n",
    "        output = self.fc2(self.dropout(hidden))\n",
    "        return output"
   ],
   "id": "3bfe90dd987072b1",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:26.051198Z",
     "start_time": "2025-04-27T19:01:26.048408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_attention_transpose():\n",
    "    # Create sample data\n",
    "    batch_size, seq_len, num_heads, head_dim = 1, 3, 2, 4\n",
    "    key = torch.ones(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "    print(\"Original key shape:\", key.shape)\n",
    "    print(\"Key before transpose:\\n\", key[0])  # Show first batch\n",
    "\n",
    "    key_transposed = key.transpose(-2, -1)\n",
    "    print(\"\\nTransposed key shape:\", key_transposed.shape)\n",
    "    print(\"Key after transpose:\\n\", key_transposed[0])  # Show first batch\n",
    "\n",
    "    return key, key_transposed\n",
    "\n",
    "# Run visualization\n",
    "key, key_t = visualize_attention_transpose()"
   ],
   "id": "ea070f6a943642fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original key shape: torch.Size([1, 2, 3, 4])\n",
      "Key before transpose:\n",
      " tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "\n",
      "Transposed key shape: torch.Size([1, 2, 4, 3])\n",
      "Key after transpose:\n",
      " tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoder Layer",
   "id": "b1f842b8f79a4c2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:28.013733Z",
     "start_time": "2025-04-27T19:01:28.011341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder layer implementing self-attention and feed-forward mechanisms.\n",
    "\n",
    "    Contains self-attention layer, feed-forward network, layer normalization,\n",
    "    and dropout for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    DROPOUT_RATE = 0.1  # Default dropout rate\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"Initialize encoder layer components.\n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimension/size\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Initialize attention mechanism\n",
    "        self.self_attention = MultiHeadAttention(num_heads, d_model)\n",
    "\n",
    "        # Initialize feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "\n",
    "        # Initialize layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Initialize dropout\n",
    "        self.dropout = nn.Dropout(self.DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, embedding, mask):\n",
    "        \"\"\"Process input through encoder layer.\n",
    "\n",
    "        Args:\n",
    "            embedding: Input embeddings\n",
    "            mask: Attention mask\n",
    "\n",
    "        Returns:\n",
    "            Processed tensor after self-attention and feed-forward layers\n",
    "        \"\"\"\n",
    "        # Self-attention block\n",
    "        attention_output = self.self_attention(embedding, embedding, embedding, mask)\n",
    "        normalized_attention = self.layer_norm(embedding + attention_output)\n",
    "\n",
    "        # Feed-forward block\n",
    "        feed_forward_output = self.feed_forward(normalized_attention)\n",
    "        feed_forward_output = self.dropout(feed_forward_output)\n",
    "\n",
    "        # Final layer normalization\n",
    "        output = self.layer_norm(normalized_attention + feed_forward_output)\n",
    "\n",
    "        return output"
   ],
   "id": "fabdfdf7dcbae344",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decoder Layer",
   "id": "360889db72f41ef9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:30.428021Z",
     "start_time": "2025-04-27T19:01:30.425615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Initialize layer components\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.encoder_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, decoder_input, encoder_output, decoder_mask, encoder_mask):\n",
    "        # Self attention block\n",
    "        self_attention = self.self_attention(\n",
    "            decoder_input, decoder_input, decoder_input, encoder_mask\n",
    "        )\n",
    "        self_attention = self.dropout(self_attention)\n",
    "        self_attention = self.layer_norm(self_attention + decoder_input)\n",
    "\n",
    "        # Encoder-decoder attention block\n",
    "        enc_dec_attention = self.encoder_attention(\n",
    "            self_attention, encoder_output, encoder_output, decoder_mask\n",
    "        )\n",
    "        enc_dec_attention = self.dropout(enc_dec_attention)\n",
    "        enc_dec_attention = self.layer_norm(enc_dec_attention + self_attention)\n",
    "\n",
    "        # Feed forward block\n",
    "        output = self.feed_forward(enc_dec_attention)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + enc_dec_attention)\n",
    "\n",
    "        return output"
   ],
   "id": "42ed168364907c3",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer",
   "id": "dae6d9e0042c2d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:32.768321Z",
     "start_time": "2025-04-27T19:01:32.765627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, word_map, max_length=50):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embedding = Embeddings(self.vocab_size, d_model, max_length)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "    def encode(self, src_words, src_mask):\n",
    "        embedding = self.embedding(src_words)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            embedding = encoder_layer(embedding, src_mask)\n",
    "        return embedding\n",
    "\n",
    "    def decode(self, tgt_words, tgt_mask, src_embeddings, src_mask):\n",
    "        embedding = self.embedding(tgt_words)\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            embedding = decoder_layer(embedding, src_embeddings,  src_mask, tgt_mask)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, src_words, src_mask, tgt_words, tgt_mask):\n",
    "        src_embeddings = self.encode(src_words, src_mask)\n",
    "        output = self.decode(tgt_words, tgt_mask, src_embeddings, src_mask)\n",
    "        return F.log_softmax(self.logit(output), dim=-1)"
   ],
   "id": "9f4c3227bf3c566",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AdamWarmup",
   "id": "d309c521302d556c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:35.089151Z",
     "start_time": "2025-04-27T19:01:35.086735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamOptimizerWithWarmup:\n",
    "    \"\"\"Implements Adam optimizer with warmup learning rate scheduling.\"\"\"\n",
    "\n",
    "    # Constants for learning rate calculation\n",
    "    MODEL_SIZE_POWER = -0.5\n",
    "    STEP_POWER = -0.5\n",
    "    WARMUP_POWER = -1.5\n",
    "\n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer wrapper.\n",
    "\n",
    "        Args:\n",
    "            model_size: Size of the model (d_model in transformer architecture)\n",
    "            warmup_steps: Number of warmup steps\n",
    "            optimizer: Base optimizer instance\n",
    "        \"\"\"\n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.learning_rate = 0.0\n",
    "\n",
    "    def calculate_learning_rate(self):\n",
    "        \"\"\"Calculate the learning rate based on current step and warmup parameters.\"\"\"\n",
    "        model_factor = self.model_size ** self.MODEL_SIZE_POWER\n",
    "        step_factor = min(\n",
    "            self.current_step ** self.STEP_POWER,\n",
    "            self.current_step * self.warmup_steps ** self.WARMUP_POWER\n",
    "        )\n",
    "        return model_factor * step_factor\n",
    "\n",
    "    def _update_optimizer_learning_rate(self, lr):\n",
    "        \"\"\"Update the learning rate in optimizer's parameter groups.\"\"\"\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Perform a single optimization step.\"\"\"\n",
    "        self.current_step += 1\n",
    "        self.learning_rate = self.calculate_learning_rate()\n",
    "        self._update_optimizer_learning_rate(self.learning_rate)\n",
    "        self.optimizer.step()"
   ],
   "id": "2d9be4a107d1d804",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:37.078380Z",
     "start_time": "2025-04-27T19:01:37.074469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LabelSmoothedLoss(nn.Module):\n",
    "    \"\"\"Loss function with label smoothing for more stable training.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, smoothing_factor):\n",
    "        \"\"\"\n",
    "        Initialize the label smoothed loss.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            smoothing_factor: Label smoothing factor between 0 and 1\n",
    "        \"\"\"\n",
    "        super(LabelSmoothedLoss, self).__init__()\n",
    "        # Initialize with 'none' reduction to handle masking manually\n",
    "        self.criterion = nn.KLDivLoss(reduction='none')\n",
    "        self.vocab_size = vocab_size\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.confidence = 1.0 - smoothing_factor\n",
    "\n",
    "    def _prepare_inputs(self, predictions, target, mask):\n",
    "        \"\"\"\n",
    "        Reshape inputs to 2D tensors for processing.\n",
    "\n",
    "        Args:\n",
    "            predictions: Model output logits\n",
    "            target: Ground truth indices\n",
    "            mask: Mask for valid positions\n",
    "\n",
    "        Returns:\n",
    "            Tuple of reshaped tensors (predictions, target, mask)\n",
    "        \"\"\"\n",
    "        predictions = predictions.contiguous().view(-1, predictions.size(-1))\n",
    "        target = target.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "        return predictions, target, mask\n",
    "\n",
    "    def _create_smoothed_labels(self, target, predictions):\n",
    "        \"\"\"\n",
    "        Create smoothed label distribution.\n",
    "\n",
    "        Args:\n",
    "            target: Ground truth indices\n",
    "            predictions: Model output logits to match shape\n",
    "\n",
    "        Returns:\n",
    "            Tensor of smoothed label distributions\n",
    "        \"\"\"\n",
    "        smoothed_labels = torch.zeros_like(predictions)\n",
    "        smoothing_value = self.smoothing_factor / (self.vocab_size - 1)\n",
    "        smoothed_labels.fill_(smoothing_value)\n",
    "        smoothed_labels.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        return smoothed_labels\n",
    "\n",
    "    def forward(self, predictions, target, mask):\n",
    "        \"\"\"\n",
    "        Compute label-smoothed loss.\n",
    "\n",
    "        Args:\n",
    "            predictions: Network output (batch_size, seq_len, vocab_size)\n",
    "            target: Ground truth indices (batch_size, seq_len)\n",
    "            mask: Mask for valid positions (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        predictions, target, mask = self._prepare_inputs(predictions, target, mask)\n",
    "        smoothed_labels = self._create_smoothed_labels(target, predictions)\n",
    "\n",
    "        # Apply log softmax and calculate KL divergence loss\n",
    "        loss = self.criterion(F.log_softmax(predictions, dim=-1), smoothed_labels)\n",
    "        masked_loss = (loss.sum(1) * mask).sum()\n",
    "        num_valid_elements = mask.sum().clamp(min=1)\n",
    "\n",
    "        return masked_loss / num_valid_elements"
   ],
   "id": "5018ea5f70ed32b",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:40.160268Z",
     "start_time": "2025-04-27T19:01:40.158558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 5\n",
    "max_words = 7\n",
    "vocab_size = 3\n",
    "prediction = torch.randn(batch_size, max_words, vocab_size)"
   ],
   "id": "71eb5e5cefb157a9",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:40.846413Z",
     "start_time": "2025-04-27T19:01:40.844292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction\n",
    "prediction.size()"
   ],
   "id": "f85a80b98fe94190",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 7, 3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:41.370569Z",
     "start_time": "2025-04-27T19:01:41.368956Z"
    }
   },
   "cell_type": "code",
   "source": "prediction = prediction.view(-1, prediction.shape[-1])",
   "id": "2cb2042e1b78e0e9",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:41.901726Z",
     "start_time": "2025-04-27T19:01:41.899669Z"
    }
   },
   "cell_type": "code",
   "source": "prediction.shape",
   "id": "35ccb3fed8898aac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:42.252019Z",
     "start_time": "2025-04-27T19:01:42.250274Z"
    }
   },
   "cell_type": "code",
   "source": "target = torch.LongTensor(batch_size * max_words).random_(0, vocab_size)",
   "id": "7344767cd76bbab6",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:42.580720Z",
     "start_time": "2025-04-27T19:01:42.578685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target\n",
    "print(target.shape, target.view(-1))"
   ],
   "id": "2203e9841ed896b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35]) tensor([0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 1, 0, 1, 2, 2, 0, 2, 2, 1,\n",
      "        2, 1, 0, 0, 0, 2, 1, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:44.778657Z",
     "start_time": "2025-04-27T19:01:44.776942Z"
    }
   },
   "cell_type": "code",
   "source": "mask = target != 0",
   "id": "595af021a67bc0dd",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:44.981089Z",
     "start_time": "2025-04-27T19:01:44.979060Z"
    }
   },
   "cell_type": "code",
   "source": "mask.shape",
   "id": "9f91bd9b5a72b617",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:45.194563Z",
     "start_time": "2025-04-27T19:01:45.193019Z"
    }
   },
   "cell_type": "code",
   "source": "labels = prediction.data.clone()",
   "id": "d0cd256a14b86f43",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:45.513169Z",
     "start_time": "2025-04-27T19:01:45.510800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels.shape\n",
    "labels.dim()"
   ],
   "id": "fc013b640d380feb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:45.947001Z",
     "start_time": "2025-04-27T19:01:45.944631Z"
    }
   },
   "cell_type": "code",
   "source": "labels[0][0]",
   "id": "871c831549bf3799",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2729)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:46.348927Z",
     "start_time": "2025-04-27T19:01:46.346150Z"
    }
   },
   "cell_type": "code",
   "source": "labels",
   "id": "c2024c2b86ea38e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2729, -0.8324, -2.0351],\n",
       "        [ 1.0763, -0.3633, -0.4272],\n",
       "        [-1.5726, -0.2920,  0.8863],\n",
       "        [-2.4818, -1.7056,  0.3215],\n",
       "        [-1.3055,  0.2403,  1.3265],\n",
       "        [ 0.9599,  0.4147,  0.6863],\n",
       "        [ 0.4083, -0.1199,  0.1647],\n",
       "        [-0.9549,  2.3450, -0.6403],\n",
       "        [ 0.9043, -1.6938, -0.3103],\n",
       "        [-0.5333,  0.6470, -1.0984],\n",
       "        [-0.0711,  1.2098,  0.9235],\n",
       "        [ 1.0407,  0.9509, -0.0578],\n",
       "        [ 1.6769,  0.0510, -0.5548],\n",
       "        [ 0.4533,  1.8790,  0.4420],\n",
       "        [-0.0061, -1.4486,  0.7430],\n",
       "        [ 0.6286, -1.1447, -0.0146],\n",
       "        [-0.3824,  1.6365,  1.0961],\n",
       "        [ 0.3965,  0.6760, -0.1092],\n",
       "        [-1.4001,  1.4434, -1.5327],\n",
       "        [ 0.7359, -1.3036, -0.1721],\n",
       "        [-1.1764, -0.7891, -1.1399],\n",
       "        [-0.9040,  0.5135,  0.0644],\n",
       "        [ 1.4838,  1.0802, -0.2865],\n",
       "        [ 1.3694,  0.6849, -1.2039],\n",
       "        [ 0.9168,  0.6265,  0.7876],\n",
       "        [-0.2294,  0.3239, -0.1895],\n",
       "        [ 0.3715, -0.0866, -0.1732],\n",
       "        [-0.0884, -0.0291,  0.0137],\n",
       "        [ 1.4637,  0.4654,  0.5072],\n",
       "        [ 1.1854,  0.5144,  0.5015],\n",
       "        [-0.5888,  0.1854,  0.2376],\n",
       "        [ 0.0916,  0.6019, -0.9831],\n",
       "        [-0.8102,  1.1569, -0.3907],\n",
       "        [-0.2789,  0.0521, -0.0052],\n",
       "        [ 0.1496, -0.6843, -0.3602]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:47.980Z",
     "start_time": "2025-04-27T19:01:47.976853Z"
    }
   },
   "cell_type": "code",
   "source": "labels.fill_(0.3 / (vocab_size - 1))",
   "id": "ec8441b1502ec3fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500],\n",
       "        [0.1500, 0.1500, 0.1500]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:50.090078Z",
     "start_time": "2025-04-27T19:01:50.088494Z"
    }
   },
   "cell_type": "code",
   "source": "# labels.scatter(1, target.data.unsqueeze(1), 1 - 0.3)",
   "id": "28ed182ab79f16a9",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:51.453996Z",
     "start_time": "2025-04-27T19:01:51.069079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_model = 512 #512\n",
    "num_heads = 8 # 8\n",
    "num_layers = 6 # 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 25 # 25\n",
    "\n",
    "with open(\"data/word_map_corpus.json\", \"r\") as f:\n",
    "    word_map = json.load(f)\n",
    "transformer = Transformer(d_model, num_heads, num_layers, word_map, 50).to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamOptimizerWithWarmup(d_model, 4000, adam_optimizer)\n",
    "criterion = LabelSmoothedLoss(vocab_size=len(word_map), smoothing_factor=0.1)"
   ],
   "id": "465f301b45c3498d",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Function",
   "id": "ba93cc2e4d22b307"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:53.878113Z",
     "start_time": "2025-04-27T19:01:53.875345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(train_loader, transformer, criterion, epoch, batch_log_frequency=100):\n",
    "    \"\"\"Train the transformer model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader containing training data\n",
    "        transformer: Transformer model to train\n",
    "        criterion: Loss function\n",
    "        epoch: Current epoch number\n",
    "        batch_log_frequency: How often to log batch progress (default: every 100 batches)\n",
    "    \"\"\"\n",
    "    transformer.train()\n",
    "    running_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    def process_batch(question, reply):\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "        batch_size = question.shape[0]\n",
    "\n",
    "        # Split reply into input and target sequences\n",
    "        reply_input = reply[:, :-1]  # all tokens except last\n",
    "        reply_target = reply[:, 1:]  # all tokens except first\n",
    "\n",
    "        # Generate masks and compute model output\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(\n",
    "            question, reply_input, reply_target)\n",
    "        model_output = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        batch_loss = criterion(model_output, reply_target, reply_target_mask)\n",
    "        return batch_loss, batch_size\n",
    "\n",
    "    def log_progress(batch_idx, running_loss, total_samples):\n",
    "        avg_loss = running_loss / total_samples\n",
    "        print(f\"Epoch {epoch} | Batch {batch_idx} | Average Loss {avg_loss:.4f}\")\n",
    "\n",
    "    for batch_idx, (question, reply) in enumerate(train_loader):\n",
    "        # Process batch and compute loss\n",
    "        batch_loss, batch_size = process_batch(question, reply)\n",
    "\n",
    "        # Update running statistics\n",
    "        running_loss += batch_loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        # Log progress at specified intervals\n",
    "        if batch_idx % batch_log_frequency == 0:\n",
    "            log_progress(batch_idx, running_loss, total_samples)"
   ],
   "id": "86f2380491d1812f",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:01:57.794619Z",
     "start_time": "2025-04-27T19:01:57.791258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_target_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a triangular (causal) mask for decoder self-attention.\n",
    "    Args:\n",
    "        size: Size of the target sequence\n",
    "    Returns:\n",
    "        Mask tensor of shape (1, 1, size, size)\n",
    "    \"\"\"\n",
    "    # Create a triangular mask (lower triangular matrix of 1's)\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
    "    # Reshape mask to (1, 1, size, size) for broadcasting\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "    return mask.to(device)\n",
    "\n",
    "def evaluate(transformer, question, question_mask, max_len=50,\n",
    "                         temperature=0.7, top_k=50):\n",
    "    \"\"\"\n",
    "    Generate response using temperature and top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        transformer: The transformer model\n",
    "        question: Input question tensor\n",
    "        question_mask: Mask for the input question\n",
    "        max_len: Maximum length of generated sequence\n",
    "        temperature: Temperature for sampling (lower = more conservative)\n",
    "        top_k: Number of top tokens to consider for sampling\n",
    "\n",
    "    Returns:\n",
    "        Generated sequence of token indices\n",
    "    \"\"\"\n",
    "    transformer.eval()\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "    # Encode the input question\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "\n",
    "    # Initialize with start token\n",
    "    words = torch.LongTensor([[word_map['<start>']]]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1):\n",
    "            # Create mask for the current sequence\n",
    "            target_mask = create_target_mask(words.size(1))\n",
    "\n",
    "            # Generate next token probabilities\n",
    "            decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "            predictions = transformer.logit(decoded[:, -1])\n",
    "\n",
    "            # Apply temperature\n",
    "            predictions = predictions / temperature\n",
    "\n",
    "            # Apply top-k filtering\n",
    "            values, indices = predictions[0].topk(top_k)\n",
    "            predictions[0] = torch.full_like(predictions[0], float('-inf'))\n",
    "            predictions[0, indices] = values\n",
    "\n",
    "            # Sample from the filtered distribution\n",
    "            probabilities = F.softmax(predictions, dim=-1)\n",
    "            next_word = torch.multinomial(probabilities, 1)\n",
    "\n",
    "            # Stop if end token is generated\n",
    "            if next_word.item() == word_map['<end>']:\n",
    "                break\n",
    "\n",
    "            # Add the new token to the sequence\n",
    "            words = torch.cat([words, next_word.view(1, 1)], dim=1)\n",
    "\n",
    "    # Convert to text\n",
    "    generated_tokens = words.squeeze(0).tolist()\n",
    "    generated_words = [rev_word_map[idx] for idx in generated_tokens\n",
    "                      if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n",
    "\n",
    "    return ' '.join(generated_words)"
   ],
   "id": "d1ef2454c05200fc",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:02:01.434622Z",
     "start_time": "2025-04-27T19:02:01.430387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def beam_search_evaluate(transformer, question, question_mask, beam_size=5, max_len=50):\n",
    "    \"\"\"\n",
    "    Generate response using beam search.\n",
    "\n",
    "    Args:\n",
    "        transformer: The transformer model\n",
    "        question: Input question tensor\n",
    "        question_mask: Mask for the input question\n",
    "        beam_size: Number of beams to maintain\n",
    "        max_len: Maximum length of generated sequence\n",
    "    \"\"\"\n",
    "    transformer.eval()\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "    # Encode the input question\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "\n",
    "    # Initialize beams with start token\n",
    "    # Each beam is (sequence, score)\n",
    "    beams = [(torch.LongTensor([[word_map['<start>']]]).to(device), 0.0)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len-1):\n",
    "            candidates = []\n",
    "\n",
    "            # Expand each current beam\n",
    "            for sequence, score in beams:\n",
    "                # Skip if sequence is already completed\n",
    "                if sequence[0][-1].item() == word_map['<end>']:\n",
    "                    candidates.append((sequence, score))\n",
    "                    continue\n",
    "\n",
    "                # Create mask for the current sequence\n",
    "                target_mask = create_target_mask(sequence.size(1))\n",
    "\n",
    "                # Generate next token probabilities\n",
    "                decoded = transformer.decode(sequence, target_mask, encoded, question_mask)\n",
    "                logits = transformer.logit(decoded[:, -1])\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "                # Get top k candidates for each beam\n",
    "                values, indices = log_probs[0].topk(beam_size)\n",
    "\n",
    "                # Create new candidates\n",
    "                for token, token_score in zip(indices, values):\n",
    "                    new_sequence = torch.cat([sequence,\n",
    "                        torch.LongTensor([[token]]).to(device)], dim=1)\n",
    "                    # Add scores in log space\n",
    "                    new_score = score + token_score.item()\n",
    "                    candidates.append((new_sequence, new_score))\n",
    "\n",
    "            # Select top beam_size candidates\n",
    "            # Sort by score and normalize by length to prevent bias towards shorter sequences\n",
    "            candidates = [(seq, score/len(seq[0])) for seq, score in candidates]\n",
    "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "            # Stop if all beams end with <end> token\n",
    "            if all(b[0][0][-1].item() == word_map['<end>'] for b in beams):\n",
    "                break\n",
    "\n",
    "    # Return the highest scoring sequence\n",
    "    best_sequence = beams[0][0]\n",
    "    generated_tokens = best_sequence.squeeze(0).tolist()\n",
    "    generated_words = [rev_word_map[idx] for idx in generated_tokens\n",
    "                      if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n",
    "\n",
    "    return ' '.join(generated_words)"
   ],
   "id": "79aae925522677b6",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T19:02:09.288009Z",
     "start_time": "2025-04-27T19:02:04.211684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(epochs):\n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'transformer_state_dict': transformer.state_dict(),\n",
    "        'optimizer_state_dict': transformer_optimizer.optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, f\"models/checkpoint_{epoch}.tar\")"
   ],
   "id": "efd32c1dce0e966b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch 0 | Average Loss 9.3603\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     state \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m: epoch,\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtransformer_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: transformer\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: transformer_optimizer\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstate_dict()\n\u001B[1;32m      7\u001B[0m     }\n\u001B[1;32m      8\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(state, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/checkpoint_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.tar\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[74], line 48\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(train_loader, transformer, criterion, epoch, batch_log_frequency)\u001B[0m\n\u001B[1;32m     46\u001B[0m transformer_optimizer\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     47\u001B[0m batch_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 48\u001B[0m \u001B[43mtransformer_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Log progress at specified intervals\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_idx \u001B[38;5;241m%\u001B[39m batch_log_frequency \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[57], line 43\u001B[0m, in \u001B[0;36mAdamOptimizerWithWarmup.step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_learning_rate()\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_optimizer_learning_rate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate)\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/conda-pycharm/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/conda-pycharm/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/.conda/envs/conda-pycharm/lib/python3.12/site-packages/torch/optim/adam.py:223\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    211\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    213\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    214\u001B[0m         group,\n\u001B[1;32m    215\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m         state_steps,\n\u001B[1;32m    221\u001B[0m     )\n\u001B[0;32m--> 223\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    233\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/.conda/envs/conda-pycharm/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/conda-pycharm/lib/python3.12/site-packages/torch/optim/adam.py:784\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 784\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/conda-pycharm/lib/python3.12/site-packages/torch/optim/adam.py:611\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    609\u001B[0m     exp_avg_sq_sqrt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 611\u001B[0m     exp_avg_sq_sqrt \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_foreach_sqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_exp_avg_sqs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    613\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001B[1;32m    614\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:48:29.687159586Z",
     "start_time": "2025-04-27T15:14:52.241405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add both custom classes and the built-in set type\n",
    "import torch.serialization\n",
    "\n",
    "# Add built-in set type\n",
    "torch.serialization.add_safe_globals([set])\n",
    "\n",
    "# Add all custom model classes\n",
    "torch.serialization.add_safe_globals([\n",
    "    Transformer,\n",
    "    MultiHeadAttention,\n",
    "    EncoderLayer,\n",
    "    DecoderLayer,\n",
    "    Embeddings,\n",
    "    FeedForward,\n",
    "    AdamOptimizerWithWarmup,\n",
    "    LabelSmoothedLoss\n",
    "])\n",
    "\n",
    "# Now try loading the checkpoint\n",
    "checkpoint = torch.load(\"models/checkpoint_24.tar\", weights_only=True)\n",
    "\n",
    "# Create new model instance and load state dict\n",
    "transformer = Transformer(d_model, num_heads, num_layers, word_map).to(device)\n",
    "transformer.load_state_dict(checkpoint['transformer_state_dict'])"
   ],
   "id": "aacc1de28f127d43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:48:29.690008074Z",
     "start_time": "2025-04-27T15:14:58.213344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Function to chat with the model using the new sampling method\n",
    "def chat_with_model(transformer, word_map, max_len=50, temperature=0.7, top_k=50):\n",
    "    \"\"\"\n",
    "    Interactive chat function with the model.\n",
    "\n",
    "    Args:\n",
    "        transformer: The transformer model\n",
    "        word_map: Dictionary mapping words to indices\n",
    "        max_len: Maximum length of generated response\n",
    "        temperature: Temperature for sampling\n",
    "        top_k: Number of top tokens to consider for sampling\n",
    "    \"\"\"\n",
    "    transformer.eval()\n",
    "    print(\"Chat started (type 'quit' to exit)\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            break\n",
    "\n",
    "        # Preprocess input\n",
    "        tokens = word_tokenize(user_input.lower())\n",
    "        encoded = [word_map.get(token, word_map['<unk>']) for token in tokens]\n",
    "\n",
    "        # Prepare input tensors\n",
    "        question = torch.LongTensor([encoded]).to(device)\n",
    "        question_mask = (question != 0).unsqueeze(1).unsqueeze(1).to(device)\n",
    "\n",
    "        # Generate response\n",
    "        response = evaluate(transformer, question, question_mask,\n",
    "                                       max_len=max_len,\n",
    "                                       temperature=temperature,\n",
    "                                       top_k=top_k)\n",
    "\n",
    "        print(\"Bot:\", response)"
   ],
   "id": "a1c497df30fc274c",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:48:29.690679698Z",
     "start_time": "2025-04-27T15:17:01.735426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# After training the model\n",
    "print(\"Starting chat with improved generation...\")\n",
    "chat_with_model(transformer, word_map,\n",
    "                max_len=50,        # Maximum response length\n",
    "                temperature=0.6,   # Lower = more focused, higher = more creative\n",
    "                top_k=10)         # Number of top tokens to consider"
   ],
   "id": "9d5762fc297e01b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat with improved generation...\n",
      "Chat started (type 'quit' to exit)\n",
      "Bot: yeah\n",
      "Bot: i\n",
      "Bot: its\n",
      "Bot: you\n",
      "Bot: i\n",
      "Bot: i\n",
      "Bot: i\n",
      "Bot: yes\n",
      "Bot: you\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# After training the model\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting chat with improved generation...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mchat_with_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Maximum response length\u001B[39;49;00m\n\u001B[1;32m      5\u001B[0m \u001B[43m                \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# Lower = more focused, higher = more creative\u001B[39;49;00m\n\u001B[1;32m      6\u001B[0m \u001B[43m                \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m         \u001B[38;5;66;03m# Number of top tokens to consider\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[42], line 18\u001B[0m, in \u001B[0;36mchat_with_model\u001B[0;34m(transformer, word_map, max_len, temperature, top_k)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChat started (type \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquit\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to exit)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# Get user input\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m     user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m user_input\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquit\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[0;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chat_with_model(transformer, word_map, max_len=50, beam_size=5):\n",
    "    transformer.eval()\n",
    "    print(\"Chat started (type 'quit' to exit)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            break\n",
    "\n",
    "        # Preprocess input\n",
    "        tokens = word_tokenize(user_input.lower())\n",
    "        encoded = [word_map.get(token, word_map['<unk>']) for token in tokens]\n",
    "\n",
    "        # Prepare input tensors\n",
    "        question = torch.LongTensor([encoded]).to(device)\n",
    "        question_mask = (question != 0).unsqueeze(1).unsqueeze(1).to(device)\n",
    "\n",
    "        # Generate response using beam search\n",
    "        response = beam_search_evaluate(transformer, question, question_mask,\n",
    "                                     beam_size=beam_size, max_len=max_len)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Try with different beam sizes\n",
    "chat_with_model(transformer, word_map, beam_size=5)  # Start with beam_size=5"
   ],
   "id": "9c5ab336b5ea8ebb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "27d577b78427631c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
