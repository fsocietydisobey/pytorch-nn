{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A Gentle Introduction to Attention and Transformer Models",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T20:19:03.557183Z",
     "start_time": "2025-03-30T20:19:02.100393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing torch for tensor creation and manipulation\n",
    "import torch\n",
    "\n",
    "# Importing torch.nn for building neural network modules\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Defining a TransformerEncoderLayer class to model one layer of the Transformer encoder\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads):\n",
    "        # Initialize the parent class nn.Module\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        # Self-attention layer with multiple heads, works on sequences with embedding size d_model\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "        # First dense layer in the feed-forward block, projecting the input embedding size (d_model) to a larger size (d_ff)\n",
    "        self.ff_proj = nn.Linear(d_model, d_ff)\n",
    "\n",
    "        # Second dense layer in the feed-forward block, projecting back from d_ff to d_model\n",
    "        self.output_proj = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # Layer normalization applied after the self-attention block\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Layer normalization applied after the feed-forward block\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ReLU activation function to introduce non-linearity in the feed-forward block\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention sublayer: x attends to itself to capture dependencies between positions in the sequence\n",
    "        residual = x  # Save residual connection for self-attention output\n",
    "        x = self.attention(x, x, x)  # Perform multi-head self-attention\n",
    "        x = self.norm1(x[0] + residual)  # Add residual connection and normalize the output\n",
    "\n",
    "        # Feed-forward sublayer: processes each position in the sequence independently\n",
    "        residual = x  # Save residual connection for feed-forward output\n",
    "        x = self.act(self.ff_proj(x))  # Apply the first dense layer and activation\n",
    "        x = self.act(self.output_proj(x))  # Apply the second dense layer and activation\n",
    "        x = self.norm2(x + residual)  # Add residual connection and normalize the output\n",
    "\n",
    "        return x  # Return the transformed sequence\n",
    "\n",
    "\n",
    "# Create a random input tensor representing a batch of 3 sequences, each of length 7, with 16 features\n",
    "seq = torch.rand(3, 7, 16)\n",
    "\n",
    "# Instantiate the TransformerEncoderLayer with input size 16, feed-forward size 32, and 4 attention heads\n",
    "layer = TransformerEncoderLayer(16, 32, 4)\n",
    "\n",
    "# Pass the input sequence through the Transformer encoder layer\n",
    "out_seq = layer(seq)\n",
    "\n",
    "# Print the shapes of all learnable parameters in the layer\n",
    "print({name: weight.shape for name, weight in layer.state_dict().items()})\n",
    "\n",
    "# Print the shape of the output sequence\n",
    "print(out_seq.shape)"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention.in_proj_weight': torch.Size([48, 16]), 'attention.in_proj_bias': torch.Size([48]), 'attention.out_proj.weight': torch.Size([16, 16]), 'attention.out_proj.bias': torch.Size([16]), 'ff_proj.weight': torch.Size([32, 16]), 'ff_proj.bias': torch.Size([32]), 'output_proj.weight': torch.Size([16, 32]), 'output_proj.bias': torch.Size([16]), 'norm1.weight': torch.Size([16]), 'norm1.bias': torch.Size([16]), 'norm2.weight': torch.Size([16]), 'norm2.bias': torch.Size([16])}\n",
      "torch.Size([3, 7, 16])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T20:34:26.727118Z",
     "start_time": "2025-03-30T20:34:26.714164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing the PyTorch library for tensor operations and deep learning utilities\n",
    "import torch\n",
    "# Importing torch.nn, which contains modules and classes for building neural networks\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Defining the TransformerEncoderLayer2 class that inherits from PyTorch's nn.Module\n",
    "# This models a single layer of the Transformer encoder block\n",
    "class TransformerEncoderLayer2(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads):\n",
    "        # Initialize the parent class nn.Module\n",
    "        super(TransformerEncoderLayer2, self).__init__()\n",
    "\n",
    "        # Multi-head attention mechanism to model relationships between sequence elements\n",
    "        # d_model: dimensionality of the input, num_heads: number of attention heads\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "        # Feed-forward block: First linear layer expands the dimensionality of input from d_model to d_ff\n",
    "        self.ff_proj = nn.Linear(d_model, d_ff)\n",
    "        # Feed-forward block: Second linear layer reduces the dimensionality back to d_model\n",
    "        self.output_proj = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        # First LayerNorm applied after self-attention\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        # Second LayerNorm applied after the feed-forward block\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # ReLU activation function to introduce non-linearity\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward method defines how the input flows through the layer\n",
    "\n",
    "        # Save the input (residual connection) for the self-attention sublayer\n",
    "        residual = x\n",
    "\n",
    "        # Normalize the input before feeding it into the attention module\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Multi-head self-attention: allows every element in the sequence to attend to every other element\n",
    "        # `x` attends to itself, considering all keys, queries, and values.\n",
    "        x = self.attention(x, x, x)\n",
    "        # Combine the output from self-attention and the residual connection\n",
    "        x = x[0] + residual\n",
    "\n",
    "        # Save the current output as a residual for the feed-forward sublayer\n",
    "        residual = x\n",
    "\n",
    "        # Normalize output before applying the feed-forward block\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        # Pass normalized data through the first feed-forward layer and apply the activation function\n",
    "        x = self.act(self.ff_proj(x))\n",
    "        # Pass the result through the second feed-forward layer and apply the activation function\n",
    "        x = self.act(self.output_proj(x))\n",
    "        # Add the residual connection to the feed-forward block's output\n",
    "        x = x + residual\n",
    "\n",
    "        # Return the final output sequence\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a random tensor to represent a batch of sequences\n",
    "# Shape: (batch size, sequence length, embedding size)\n",
    "seq = torch.rand(3, 7, 16)\n",
    "\n",
    "# Instantiate the Transformer layer with:\n",
    "# d_model = 16 (input size), d_ff = 32 (feed-forward size), num_heads = 4 (attention heads)\n",
    "layer = TransformerEncoderLayer2(16, 32, 4)\n",
    "\n",
    "# Pass the input sequence through the Transformer encoder layer and compute the output\n",
    "out_seq = layer(seq)\n",
    "\n",
    "# Print the shapes of all learnable parameters (weights and biases) in the layer\n",
    "print({name: weight.shape for name, weight in layer.state_dict().items()})\n",
    "\n",
    "# Print the shape of the final output tensor after processing through the layer\n",
    "print(out_seq.shape)"
   ],
   "id": "84256b132f41d52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention.in_proj_weight': torch.Size([48, 16]), 'attention.in_proj_bias': torch.Size([48]), 'attention.out_proj.weight': torch.Size([16, 16]), 'attention.out_proj.bias': torch.Size([16]), 'ff_proj.weight': torch.Size([32, 16]), 'ff_proj.bias': torch.Size([32]), 'output_proj.weight': torch.Size([16, 32]), 'output_proj.bias': torch.Size([16]), 'norm1.weight': torch.Size([16]), 'norm1.bias': torch.Size([16]), 'norm2.weight': torch.Size([16]), 'norm2.bias': torch.Size([16])}\n",
      "torch.Size([3, 7, 16])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T20:44:01.888234Z",
     "start_time": "2025-03-30T20:44:01.878150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This code defines and demonstrates the usage of a TransformerDecoderLayer class.\n",
    "\n",
    "import torch  # Import PyTorch for tensor and deep learning operations\n",
    "import torch.nn as nn  # Import neural network functionality from PyTorch\n",
    "\n",
    "\n",
    "# Define the TransformerDecoderLayer class, modeling a single layer of the Transformer decoder\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads):\n",
    "        super(TransformerDecoderLayer, self).__init__()  # Initialize the base class nn.Module\n",
    "        # Multi-head self-attention layer processes the decoder sequence by allowing each element to attend to all others\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        # Multi-head cross-attention layer attends to the encoder's output for generating context-aware decoder outputs\n",
    "        self.xattention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        # First feed-forward layer increases representation dimensionality from d_model to d_ff\n",
    "        self.ff_proj = nn.Linear(d_model, d_ff)\n",
    "        # Second feed-forward layer reduces representation dimensionality back from d_ff to d_model\n",
    "        self.output_proj = nn.Linear(d_ff, d_model)\n",
    "        # Layer normalization (norm1) stabilizes the output of the self-attention mechanism\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        # Layer normalization (norm2) stabilizes the output of cross-attention\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Layer normalization (norm3) stabilizes the output of the feed-forward network\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        # ReLU activation function introduces non-linear transformations for the feed-forward network\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Forward pass defines how input tensors flow through the TransformerDecoderLayer\n",
    "\n",
    "        # Self-attention sublayer: x attends to itself for intra-sequence dependencies\n",
    "        residual = x  # Preserve input x as residual for skip connection\n",
    "        x = self.norm1(x)  # Apply layer normalization to stabilize input\n",
    "        x = self.attention(x, x, x)  # Compute multi-head self-attention\n",
    "        x = x[0] + residual  # Add skip connection (residual) to self-attention output\n",
    "\n",
    "        # Cross-attention sublayer: attends to the encoder's output (context)\n",
    "        residual = x  # Preserve current value of x for residual connection\n",
    "        x = self.norm2(x)  # Normalize input before cross-attention\n",
    "        x = self.xattention(x, y, y)  # Compute cross-attention with encoder outputs (keys and values)\n",
    "        x = x[0] + residual  # Add residual connection to cross-attention output\n",
    "\n",
    "        # Feed-forward sublayer: introduces non-linear transformations and higher representational power\n",
    "        residual = x  # Preserve current x for another skip connection\n",
    "        x = self.norm3(x)  # Normalize input before passing through feed-forward network\n",
    "        x = self.act(self.ff_proj(x))  # Pass normalized input through first feed-forward layer and apply ReLU\n",
    "        x = self.act(self.output_proj(x))  # Pass through second feed-forward layer and apply ReLU\n",
    "        x = x + residual  # Add residual connection to feed-forward output\n",
    "\n",
    "        return x  # Return the fully processed tensor\n",
    "\n",
    "\n",
    "# Create a random input tensor for the decoder sequence\n",
    "# Shape: (batch_size=3, seq_len=7, embedding size=16)\n",
    "dec_seq = torch.rand(3, 7, 16)\n",
    "\n",
    "# Create a random input tensor to represent the encoder's output sequence\n",
    "# Shape: (batch_size=3, seq_len=11, embedding size=16)\n",
    "enc_seq = torch.rand(3, 11, 16)\n",
    "\n",
    "# Instantiate a TransformerDecoderLayer with model dim=16, ff dim=32, 4 attention heads\n",
    "layer = TransformerDecoderLayer(16, 32, 4)\n",
    "\n",
    "# Forward pass: Process the decoder sequence (dec_seq) and encoder's output (enc_seq) through the layer\n",
    "out_seq = layer(dec_seq, enc_seq)\n",
    "\n",
    "# Print the shapes of all learnable parameters in the TransformerDecoderLayer\n",
    "print({name: weight.shape for name, weight in layer.state_dict().items()})\n",
    "\n",
    "# Print the shape of the final output tensor\n",
    "print(out_seq.shape)"
   ],
   "id": "1e326dd2e8db76d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention.in_proj_weight': torch.Size([48, 16]), 'attention.in_proj_bias': torch.Size([48]), 'attention.out_proj.weight': torch.Size([16, 16]), 'attention.out_proj.bias': torch.Size([16]), 'xattention.in_proj_weight': torch.Size([48, 16]), 'xattention.in_proj_bias': torch.Size([48]), 'xattention.out_proj.weight': torch.Size([16, 16]), 'xattention.out_proj.bias': torch.Size([16]), 'ff_proj.weight': torch.Size([32, 16]), 'ff_proj.bias': torch.Size([32]), 'output_proj.weight': torch.Size([16, 32]), 'output_proj.bias': torch.Size([16]), 'norm1.weight': torch.Size([16]), 'norm1.bias': torch.Size([16]), 'norm2.weight': torch.Size([16]), 'norm2.bias': torch.Size([16]), 'norm3.weight': torch.Size([16]), 'norm3.bias': torch.Size([16])}\n",
      "torch.Size([3, 7, 16])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fe002a5e41c75b8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
