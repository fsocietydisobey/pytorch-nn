{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T23:47:00.620358Z",
     "start_time": "2025-03-13T23:46:59.257689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/345/345-0.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the content to a file\n",
    "with open(\"data/text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "print(\"Dracula text downloaded and saved as dracula.txt\")\n"
   ],
   "id": "c85d0a6e452761db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dracula text downloaded and saved as dracula.txt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter, \\\n",
    "    defaultdict  # Import utilities for character frequency counting and default dictionary creation.\n",
    "\n",
    "import numpy as np  # Import for numerical operations.\n",
    "import pandas as pd  # Import for handling tabular data.\n",
    "import torch  # Import PyTorch framework for building deep learning models.\n",
    "import torch.nn as nn  # Import neural network modules from PyTorch.\n",
    "import torch.nn.functional as F  # Import functional methods for building layers/operations.\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler  # Tools for data handling and loading in PyTorch.\n",
    "from tqdm import tqdm  # Import for progress bar visualization during training loops.\n",
    "\n",
    "\n",
    "class CharacterDataset(Dataset):\n",
    "    \"\"\"Custom class for creating a dataset of character sequences.\n",
    "\n",
    "    Useful for feeding a sequence of characters and their corresponding outputs to a neural network.\n",
    "    \"\"\"\n",
    "    \"\"\"Custom dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text that will be used to create the entire database.\n",
    "\n",
    "    window_size : int\n",
    "        Number of characters to use as input features.\n",
    "\n",
    "    vocab_size : int\n",
    "        Number of characters in the vocabulary. Note that the last character\n",
    "        is always reserved for a special \"~\" out-of-vocabulary character.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    ch2ix : defaultdict\n",
    "        Mapping from the character to the position of that character in the\n",
    "        vocabulary. Note that all characters that are not in the vocabulary\n",
    "        will get mapped into the index `vocab_size - 1`.\n",
    "\n",
    "    ix2ch : dict\n",
    "        Mapping from the character position in the vocabulary to the actual\n",
    "        character.\n",
    "\n",
    "    vocabulary : list\n",
    "        List of all characters. `len(vocabulary) == vocab_size`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text, window_size=1, vocab_size=50):\n",
    "        self.text = text.replace(\"\\n\",\n",
    "                                 \" \")  # Replace newline characters in the input text with spaces to unify text formatting.\n",
    "        self.window_size = window_size\n",
    "        self.ch2ix = defaultdict(lambda: vocab_size - 1)\n",
    "\n",
    "        # Create a dictionary mapping each of the `vocab_size - 1` most common characters\n",
    "        # in the input text to a unique index. The character itself is the key (x[0]),\n",
    "        # and the assigned index (given by `enumerate`) is the value.\n",
    "        # Any character not in this mapping will default to the out-of-vocabulary index.\n",
    "        most_common_ch2ix = {  # Map the `vocab_size-1` most common characters to unique indices for model input.\n",
    "            x[0]: i\n",
    "            for i, x in enumerate(Counter(self.text).most_common()[: (vocab_size - 1)])\n",
    "        }\n",
    "        # Update the mapping \"ch2ix\" (character to index) with the most common characters.\n",
    "        self.ch2ix.update(most_common_ch2ix)\n",
    "\n",
    "        # Assign the special character '~' to the index `vocab_size - 1`,\n",
    "        # which acts as a placeholder for any out-of-vocabulary characters.\n",
    "        self.ch2ix[\"~\"] = vocab_size - 1\n",
    "\n",
    "        # Create a reverse mapping \"ix2ch\" (index to character) from the \"ch2ix\" dictionary,\n",
    "        # enabling easy lookup of characters by their assigned index.\n",
    "        self.ix2ch = {v: k for k, v in self.ch2ix.items()}\n",
    "\n",
    "        # Generate the vocabulary list by mapping each index `i` to its corresponding\n",
    "        # character from the \"ix2ch\" dictionary for all indices in the range of `vocab_size`.\n",
    "        # This list represents all characters included in the model's vocabulary.\n",
    "        self.vocabulary = [self.ix2ch[i] for i in range(vocab_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(\n",
    "            self.text) - self.window_size  # Return the number of samples in the dataset (length of text minus window size).\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        X = torch.LongTensor(\n",
    "            # Convert the sequence of characters to a numerical representation (IDs from the vocabulary mapping).\n",
    "            [self.ch2ix[c] for c in self.text[ix: ix + self.window_size]]\n",
    "        )\n",
    "        y = self.ch2ix[self.text[ix + self.window_size]]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    # Define the model structure, combining embedding, LSTM, and dense layers for character-level text generation.\n",
    "    \"\"\"\n",
    "    Custom network for predicting the next character of a string or handling multi-class tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        Number of characters in the vocabulary.\n",
    "    embedding_dim : int\n",
    "        Dimension of the character embedding vectors.\n",
    "    dense_dim : int\n",
    "        Number of neurons in the intermediate dense layer.\n",
    "    hidden_dim : int\n",
    "        Size of the LSTM hidden state.\n",
    "    max_norm : int, optional\n",
    "        Maximum L2 norm of the embedding vectors, default is 2.\n",
    "    n_layers : int, optional\n",
    "        Number of LSTM layers, default is 1.\n",
    "    dropout_rate : float, optional\n",
    "        Probability of dropout applied after the LSTM and dense layer, default is 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_dim=64,\n",
    "            dense_dim=64,\n",
    "            hidden_dim=128,\n",
    "            max_norm=2,\n",
    "            n_layers=1,\n",
    "            dropout_rate=0.5,\n",
    "    ):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # Validations\n",
    "        if vocab_size < 1:\n",
    "            raise ValueError(\"`vocab_size` must be >= 1\")\n",
    "        if max_norm <= 0:\n",
    "            raise ValueError(\"`max_norm` must be positive\")\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise ValueError(\"`dropout_rate` must be between 0 and 1\")\n",
    "\n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(  # Embedding layer to map characters to dense vector representations.\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=vocab_size - 1,  # Assume last index in vocab is for padding\n",
    "            max_norm=max_norm,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear_1 = nn.Linear(hidden_dim, dense_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.linear_2 = nn.Linear(dense_dim, vocab_size)\n",
    "\n",
    "        # Hidden state initialization\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape `(batch_size, sequence_length)` with dtype `torch.int64`.\n",
    "        hidden : tuple of torch.Tensor, optional  # Previous hidden and cell states of the LSTM, used for sequential processing.\n",
    "            Tuple of (hidden_state, cell_state), each of shape `(n_layers, batch_size, hidden_dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Output tensor of shape `(batch_size, vocab_size)` with unnormalized scores for each class.\n",
    "        hidden : tuple of torch.Tensor\n",
    "            Updated hidden states from the LSTM.\n",
    "        \"\"\"\n",
    "        x = x.to(\n",
    "            next(self.parameters()).device)  # Move input to the same device (CPU or GPU) as the model's parameters.\n",
    "\n",
    "        # Initialize hidden states if not provided\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(x.size(0))\n",
    "\n",
    "        # Embedding layer\n",
    "        embeddings = self.embedding(x)  # Create dense vector representations of the input characters.\n",
    "\n",
    "        # LSTM layer\n",
    "        _, hidden = self.lstm(embeddings,\n",
    "                              hidden)  # Process the input embeddings through the LSTM and update hidden states.\n",
    "\n",
    "        # Process last hidden state of all LSTM layers\n",
    "        last_hidden = hidden[0].mean(dim=0)  # Take mean across LSTM layers -> Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # Fully connected layers\n",
    "        dense_output = self.activation(\n",
    "            self.linear_1(last_hidden))  # Apply a linear transformation followed by ReLU activation.\n",
    "        dense_output = self.dropout(dense_output)  # Apply dropout\n",
    "        logits = self.linear_2(dense_output)  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "        return logits, hidden  # Return the predicted logits and updated hidden states.\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden states for the LSTM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The batch size of the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden : tuple of torch.Tensor\n",
    "            Initial hidden_state and cell_state, both of shape `(n_layers, batch_size, hidden_dim)`.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device  # Determine the device (CPU or GPU) to initialize the hidden states.\n",
    "        return (\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device),\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device)\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_loss(cal, net, dataloader):\n",
    "    \"\"\"Computer average loss over a dataset.\"\"\"\n",
    "    net.eval()\n",
    "    all_losses = []\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        probs, _, _ = net(X_batch)\n",
    "\n",
    "        all_losses.append(cal(probs, y_batch).item())\n",
    "\n",
    "    return np.mean(all_losses)  # Calculate and return the average loss over the dataset.\n",
    "\n",
    "\n",
    "def generate_text(n_chars, net, dataset, initial_text=\"Hello\", random_state=None):\n",
    "    \"\"\"Generate text with the character-level model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        Number of characters to generate.\n",
    "\n",
    "    net : Module\n",
    "        Character-level model.\n",
    "\n",
    "    dataset : CharacterDataset\n",
    "        Instance of the `CharacterDataset`.\n",
    "\n",
    "    initial_text : str\n",
    "        The starting text to be used as the initial condition for the model.\n",
    "\n",
    "    random_state : None or int\n",
    "        If not None, then the result is reproducible.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : str\n",
    "        Generated text.\n",
    "    \"\"\"\n",
    "    if not initial_text:\n",
    "        raise ValueError(\"You need to specify the initial text\")\n",
    "\n",
    "    res = initial_text  # Start generating text with the user-specified initial string.\n",
    "    net.eval()\n",
    "    h, c = None, None\n",
    "\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    for _ in range(n_chars):  # Generate the specified number of characters, updating the initial text.\n",
    "        previous_chars = initial_text if res == initial_text else res[-1]\n",
    "        features = torch.LongTensor([[dataset.ch2ix[c] for c in previous_chars]])\n",
    "        logits, h, c = net(features, h, c)\n",
    "        probs = F.softmax(logits[0],\n",
    "                          dim=0).detach().numpy()  # Apply softmax to obtain probabilities for each character in the vocabulary.\n",
    "        new_ch = np.random.choice(dataset.vocabulary, p=probs)\n",
    "        res += new_ch\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"text.txt\", \"r\") as f:  # Read the training text dataset from the file.\n",
    "        text = \"\\n\".join(f.readlines())\n",
    "\n",
    "    # Hyperparameters model\n",
    "    vocab_size = 70\n",
    "    window_size = 10\n",
    "    embedding_dim = 2\n",
    "    hidden_dim = 16\n",
    "    dense_dim = 32\n",
    "    n_layers = 1\n",
    "    max_norm = 2\n",
    "\n",
    "    # Training config\n",
    "    n_epochs = 25  # Number of epochs the model will train for.\n",
    "    train_val_split = 0.8\n",
    "    batch_size = 128\n",
    "    random_state = 13\n",
    "\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    dataset = CharacterDataset(text, window_size=window_size,\n",
    "                               vocab_size=vocab_size)  # Prepare the character-level dataset.\n",
    "\n",
    "    n_samples = len(dataset)\n",
    "    split_ix = int(n_samples * train_val_split)\n",
    "\n",
    "    train_indices, val_indices = np.arange(split_ix), np.arange(split_ix, n_samples)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, sampler=SubsetRandomSampler(train_indices), batch_size=batch_size\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        dataset, sampler=SubsetRandomSampler(val_indices), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    net = Network(\n",
    "        vocab_size,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_layers=n_layers,\n",
    "        dense_dim=dense_dim,\n",
    "        embedding_dim=embedding_dim,\n",
    "        max_norm=max_norm,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr=1e-2,\n",
    "    )\n",
    "\n",
    "    emb_history = []\n",
    "\n",
    "    for e in range(n_epochs + 1):\n",
    "        net.train()\n",
    "        for X_batch, y_batch in tqdm(train_dataloader):\n",
    "            if e == 0:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            probs, _, _ = net(X_batch)\n",
    "            loss = loss_f(probs, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = compute_loss(loss_f, net, train_dataloader)\n",
    "        val_loss = compute_loss(loss_f, net, val_dataloader)\n",
    "        print(f\"Epoch: {e}, {train_loss=:.3f}, {val_loss=:.3f}\")\n",
    "\n",
    "        # Generate one sentence\n",
    "        initial_text = \"I hope it works \"\n",
    "        generated_text = generate_text(\n",
    "            100, net, dataset, initial_text=initial_text, random_state=random_state\n",
    "        )\n",
    "        print(generated_text)\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        weights = net.embedding.weight.detach().clone().numpy()\n",
    "\n",
    "        df = pd.DataFrame(weights, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\n",
    "        df[\"epoch\"] = e\n",
    "        df[\"character\"] = dataset.vocabulary\n",
    "\n",
    "        emb_history.append(df)\n",
    "\n",
    "final_df = pd.concat(emb_history)\n",
    "final_df.to_csv(\"res.csv\", index=False)  # Save the history of embeddings over epochs to a CSV file."
   ],
   "id": "fbc121e30a2defb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TextClassificationLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=1, dropout=0.5):\n",
    "        super(TextClassificationLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)  # Intermediate hidden-to-hidden layer\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # Final hidden-to-output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        _, (hidden, _) = self.lstm(x)  # hidden: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # Take the last hidden state from the last LSTM layer\n",
    "        hidden = hidden[-1]  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # Apply dropout to the hidden layer output\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.fc2(x)  # Output logits for num_classes\n",
    "\n",
    "        return x\n"
   ],
   "id": "48bee304af012231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LanguageModelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        super(LanguageModelLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        x, hidden = self.lstm(x, hidden)  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Apply dropout to the hidden layer output\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten the LSTM output for the FC layer\n",
    "        x = x.reshape(-1, x.size(2))  # Shape: (batch_size * seq_len, hidden_dim)\n",
    "\n",
    "        # Fully connected layer for logits\n",
    "        x = self.fc(x)  # Shape: (batch_size * seq_len, vocab_size)\n",
    "\n",
    "        return x, hidden\n"
   ],
   "id": "a028dbe8cb0716a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected layer for the decoder to map hidden states to output vocab\n",
    "        self.fc = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, trg, hidden=None):\n",
    "        # Encoder\n",
    "        src = self.encoder_embedding(src)  # Shape: (batch_size, src_len, embedding_dim)\n",
    "        _, hidden = self.encoder_lstm(src, hidden)  # hidden: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "        # Decoder\n",
    "        trg = self.decoder_embedding(trg)  # Shape: (batch_size, trg_len, embedding_dim)\n",
    "        x, hidden = self.decoder_lstm(trg, hidden)  # x: (batch_size, trg_len, hidden_dim)\n",
    "\n",
    "        # Apply dropout to decoder LSTM outputs (before FC layer)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Fully connected layer to output vocabulary logits\n",
    "        x = self.fc(x)  # Shape: (batch_size, trg_len, output_vocab_size)\n",
    "\n",
    "        return x\n"
   ],
   "id": "8dcdf2e7d8628db6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.5):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # LSTM layer\n",
    "        x, hidden = self.lstm(x, hidden)  # x: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Only take the last hidden state for prediction\n",
    "        x = x[:, -1, :]  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        # Apply dropout before the fully connected layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Fully connected layer to produce predictions\n",
    "        x = self.fc(x)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        return x, hidden\n"
   ],
   "id": "51f8de8852f391a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The decision of whether to apply dropout between `fc1` and `fc2` in the **TextClassificationLSTM** model depends on how you want to regularize the model and which part of the network you want to prevent from overfitting. Let’s break this down conceptually so it makes sense why I chose to apply dropout before `fc1` and why it could (or could not) also be applied between `fc1` and `fc2`.\n",
    "### **Where Dropout Is Currently Applied**\n",
    "In the corrected example, dropout is applied **before `fc1`**, which means dropout acts on the high-dimensional representation extracted from the LSTM’s hidden states. This is the critical feature space that carries most of the information extracted from the sequenced input, so regularizing this representation has the biggest impact on generalization.\n",
    "By regularizing these features **before any dense layer**, we ensure:\n",
    "1. We prevent over-reliance on specific neurons in the hidden LSTM representation.\n",
    "2. `fc1` processes robust features that have already been regularized.\n",
    "\n",
    "This strategic placement ensures the dense transformation (`fc1`) is working on stabilized inputs.\n",
    "### **Why Not Apply Dropout Between `fc1` and `fc2`?**\n",
    "#### 1. **Prevent Excessive Regularization**\n",
    "Including dropout **both before `fc1` and between `fc1` and `fc2` could result in excessive regularization**:\n",
    "- Dropout in multiple places along a small MLP (multi-layer perceptron) can hurt the model's ability to learn detailed patterns because too many features are \"dropped out\" at various stages.\n",
    "- For smaller models like this, it’s generally sufficient to apply dropout **once** at the input to the fully connected layers (directly after the LSTM in this case).\n",
    "\n",
    "#### 2. **fc1 is Acting as a Feature Reduction Layer**\n",
    "The first fully connected layer (`fc1`) is reducing the dimensionality of the hidden state output from the LSTM (e.g., from `hidden_dim` → 64 in the example). This feature reduction itself acts as a type of regularization because it forces the model to condense the information into a smaller space. Applying dropout after the dimensionality has already been reduced may not add much benefit.\n",
    "#### 3. **Dropout Is Computationally Expensive**\n",
    "Dropout adds some computational cost during training. Applying it too frequently in smaller architectures, especially in simple MLP chains, will generally result in diminishing returns since the primary goal of dropout—to regularize activations—has already been achieved after the LSTM output stage.\n",
    "#### 4. **It's More Common to Regularize Larger Feature Spaces**\n",
    "Dropout is most effective when applied to **high-dimensional feature spaces**, such as the outputs of an LSTM, convolutional layers, or very large dense layers. Since the output of `fc1` (64 units in the example) is already compressed, the likelihood of overfitting at this stage is lower, so dropout may not have a major impact here.\n",
    "### **When Should You Apply Dropout Between `fc1` and `fc2`?**\n",
    "While the design choice not to apply dropout between `fc1` and `fc2` is generally fine for the current model, there are situations where it might make sense to do so:\n",
    "1. **Larger Models with Deeper FC Layers**\n",
    "    - If the fully connected section of your network consists of multiple dense layers (e.g., `fc1`, `fc2`, `fc3`...), then applying dropout between layers can help regularize each transformation in this deeper part of the model.\n",
    "    - Example:\n",
    "``` python\n",
    "x = self.fc1(hidden)\n",
    "x = self.relu(x)\n",
    "x = self.dropout(x)  # Dropout applied between fc1 and fc2\n",
    "x = self.fc2(x)\n",
    "```\n",
    "1. **When Overfitting is Observed**\n",
    "    - If validation performance significantly lags behind training performance in your current model, you can experiment with adding dropout between dense layers to further reduce overfitting.\n",
    "\n",
    "2. **Higher Dimensional Representations**\n",
    "    - If `fc1` has a much larger output dimension (e.g., reducing from 1024 → 512 → 256), then applying dropout between dense layers (e.g., after `fc1`) can help ensure that intermediate representations don’t overfit.\n",
    "\n",
    "### **Updated TextClassificationLSTM Example with Dropout Between fc1 and fc2**\n",
    "Here’s how the model would look if dropout **is applied between `fc1` and `fc2`**:\n",
    "``` python\n",
    "class TextClassificationLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=1, dropout=0.5):\n",
    "        super(TextClassificationLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)  # Reduce dimensionality\n",
    "        self.relu = nn.ReLU()  # Activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # Map to output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        _, (hidden, _) = self.lstm(x)  # hidden: (num_layers, batch_size, hidden_dim)\n",
    "        hidden = hidden[-1]  # Take the last hidden state: (batch_size, hidden_dim)\n",
    "\n",
    "        # Pass hidden state through two FC layers\n",
    "        x = self.fc1(hidden)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # Dropout applied between FC layers\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "```\n",
    "### **Key Takeaways**\n",
    "- Dropout placement depends on how rich and high-dimensional the feature space is:\n",
    "    - Apply **dropout once on the largest transformations** (e.g., the LSTM output).\n",
    "    - Adding dropout **between smaller dense layers** might help if overfitting is still present.\n",
    "\n",
    "- Avoid **over-regularizing** small models by applying dropout too frequently, as it can degrade performance.\n",
    "- Test dropout placement experimentally if you're unsure—it’s often a hyperparameter choice that depends on the task and dataset.\n"
   ],
   "id": "e1d035e4dd9f3b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ac97f3f191b94a11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
